%%
%% JSAI2026 Content - Transformer geDIG (4ページ拡張版)
%%

\title{
\jtitle{Attention機構の熱力学的解釈：geDIGゲージによる構造評価とF正則化}
\etitle{Thermodynamic Interpretation of Attention: Structural Evaluation via geDIG Gauge and F-Regularization}
}

\jaddress{宮内 和義（独立研究者），所在地：日本，E-mail: miyauchikazuyoshi@gmail.com}

\author{%
\jname{宮内 和義\first}
\ename{Kazuyoshi Miyauchi}
}

\affiliate{
\jname{\first{}独立研究者}
\ename{Independent Researcher}
}

\begin{abstract}
Transformer attention lacks a unified scalar metric for quality. We propose the geDIG gauge, which interprets attention as a directed graph and operationally bridges the Free Energy Principle and the Minimum Description Length by trading off structural cost (edit-path cost) against information gain (entropy reduction and relative shortest-path gain): $\F = \gednorm - \lambda(\Delta H_{\mathrm{norm}} + \gamma \Delta \mathrm{SP}_{\mathrm{rel}})$. On BERT/GPT-2, real attention is more structured than random baselines and shows a sharp layer-wise transition. The same tendency is reproduced on Llama 3.0/3.1, while it is weaker on Phi-3. Finally, adding $\F$ as a regularization term in DistilBERT fine-tuning on SST-2 yields a small improvement under weak regularization, suggesting the gauge can serve as an actionable factor to intervene in internal structure.
\end{abstract}

\begin{document}
\maketitle

%% ===========================================
\section{はじめに}
%% ===========================================

Transformer\cite{vaswani2017}は自然言語処理の基盤モデルとして広く普及しているが，Attention機構の「質」を定量評価する統一指標は確立されていない．
既存研究ではヘッドのpruning基準\cite{voita2019}や層の役割解釈\cite{clark2019}が個別に議論されているが，「なぜそのAttentionパターンが良いのか」を原理的に説明する枠組みは不足している．

本研究では，Attentionパターンを有向グラフとして構築し，自由エネルギー原理（FEP）\cite{friston2010}と最小記述長（MDL）\cite{grunwald2007}を橋渡しするgeDIGゲージで評価する手法を提案する．
geDIGは元来，動的知識グラフにおける「いつ新しい知識を受け入れるか（When）」という判断基準として設計された指標である．
本研究では，この枠組みをTransformerの内部表現に適用し，「Attention層は情報の相転移的な遷移を示す可能性がある」という仮説を検証する．

本研究の貢献は以下の3点である：
\begin{enumerate}
\item geDIGの理論的背景（FEP-MDL橋渡し）とAttentionへの適用手法の提示
\item 実AttentionがRandomより構造化されていること，および層別の遷移的挙動の観測
\item F正則化による因果的示唆（性能向上の観測）
\end{enumerate}

%% ===========================================
\section{geDIGの理論的背景}
%% ===========================================

\subsection{設計前提と直観}

geDIGは「構造を更新するか否か」を判断するためのゲージであり，以下の前提に立つ：
\begin{enumerate}
\item \textbf{構造はグラフで表現できる}：知識や関係は頂点と辺で表され，編集はエッジ追加・削除として表現できる．
\item \textbf{更新にはコストがある}：構造を変えるほど編集コスト（複雑さ）が増え，過剰な更新は不利である．
\item \textbf{更新には価値がある}：良い更新は不確実性を減らし，経路を短縮して推論効率を高める．
\end{enumerate}

この3点を同時に満たすとき，「コストと利得の釣り合い」を単一スカラーで扱える．
geDIGはそのための統一ゲージである．

\noindent\textbf{(1) 直観例：ショートカット追加}

鎖状グラフに1本のショートカットを追加する場合，編集コスト（$\gednorm$）は増えるが，
平均最短路の改善（$\Delta \mathrm{SP}_{\mathrm{rel}}$）が大きければ，総合的には「構造が良くなった」と判断される．
一方，無秩序に多数のエッジを追加しても経路短縮が起きなければ，コストだけが増えて不利になる．
geDIGはこの直観を形式化し，構造編集の是非を数値で判定できるようにする．

\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{figs/gedig_intuition.pdf}
\caption{geDIGの直観的解釈：ショートカット追加で編集コストが増える一方，平均最短路が短縮される．}
\label{fig:gedig_intuition}
\end{figure}

\subsection{問題設定：「When」の判断基準}

動的に変化する知識グラフ（または情報構造）に新たな要素が注入される場面を考える．
このときシステムは，その要素を統合すべきか，棄却すべきかを判断しなければならない．
この判断には，\textbf{構造編集によるコスト}と\textbf{情報利得}の天秤が必要である．

従来のRAG（Retrieval-Augmented Generation）は「何を取るか（What）」の最適化に長けるが，「いつ受け入れるか（When）」の規範を欠く\cite{lewis2020rag}．
geDIGはこのギャップを埋めるために設計された統一ゲージである．

\subsection{自由エネルギー原理とMDLの橋渡し}

geDIGは2つの理論的枠組みを操作的に橋渡しする：

\noindent\textbf{自由エネルギー原理（FEP）}：
Fristonの自由エネルギー原理\cite{friston2010}は，生物システムが「予測誤差（驚き）」を最小化するように行動・学習するという枠組みである．
geDIGでは，エントロピー項 $\Delta H$ が「情報の曖昧さ・不確実性」に対応し，これを低減することが「秩序化」として解釈される．

\noindent\textbf{最小記述長（MDL）}：
MDL原理\cite{grunwald2007}は，データとモデルの総記述長 $L(M) + L(D|M)$ を最小化する枠組みである．
geDIGでは，構造コスト $\gednorm$ が $L(M)$（モデルの複雑さ）に，情報利得 $\ignorm$ が $L(D|M)$（データの圧縮効率）に対応する．

この対応により，geDIGは「構造の複雑さ」と「情報の整理度」のトレードオフを単一スカラーで評価する．

\subsection{統一ゲージの定義}

geDIGゲージ $\F$ を以下のように定義する：
\begin{equation}
\F = \gednorm - \lambda \left( \Delta H_{\mathrm{norm}} + \gamma \cdot \Delta \mathrm{SP}_{\mathrm{rel}} \right)
\label{eq:gedig}
\end{equation}

ここで $\ignorm = \Delta H_{\mathrm{norm}} + \gamma \cdot \Delta \mathrm{SP}_{\mathrm{rel}}$ とおくと，
$\F = \gednorm - \lambda \ignorm$ と書ける．
各項の意味を表\ref{tab:gedig_terms}に示す．

\begin{table}[t]
\centering
\caption{geDIGの各項の意味と理論的対応}
\label{tab:gedig_terms}
\small
\begin{tabular}{llll}
\toprule
項 & 意味 & 理論対応 & 符号 \\
\midrule
$\gednorm$ & 編集経路コスト & MDL: $L(M)$ & 大→悪 \\
$\Delta H_{\mathrm{norm}}$ & エントロピー差 & FEP: 驚き & 負→秩序化 \\
$\Delta \mathrm{SP}_{\mathrm{rel}}$ & 相対経路ゲイン & MDL: 圧縮 & 正→効率化 \\
$\lambda, \gamma$ & 重み係数 & 情報温度 & -- \\
\bottomrule
\end{tabular}
\end{table}

\textbf{解釈}：本実装では$\F$が負の範囲に収まりやすく，\textbf{絶対値よりもベースラインとの差}が重要である．
実AttentionはRandomより$\F$が0に近づき，\textbf{より構造化された状態}と解釈する．
したがって本稿では $\Delta F = F_{\mathrm{real}} - F_{\mathrm{random}}$ を主要な比較指標として扱う．

\subsection{二段ゲート：AG/DG}

geDIGは本来，0-hopと multi-hopの二段階で評価を行う：

\noindent\textbf{AG（Attention Gate）}：0-hop評価．
局所的な編集直後の状態を評価し，「曖昧さ・不確実性」を検知する．
$g_0 > \theta_{\mathrm{AG}}$ ならば探索を深化させる（FEP的な予測誤差最小化）．

\noindent\textbf{DG（Decision Gate）}：Multi-hop評価．
複数ステップ先までの構造効率を評価し，「短絡（ショートカット）」の形成を確認する．
$g_{\min} < \theta_{\mathrm{DG}}$ ならば統合を確定する（MDL的な記述長削減）．

本研究のAttention適用では，この二段構造を「層をまたいだ情報の流れ」として再解釈する．

%% ===========================================
\section{関連研究と位置づけ}
%% ===========================================

Attentionの解析や重要度評価に関しては，ヘッドの可視化や役割分担の分析\cite{clark2019}，
および剪定（pruning）に向けた重要度推定\cite{voita2019}が広く用いられてきた．
これらは有用だが，\textbf{なぜそのAttentionが良いのか}を説明する原理的指標は明確でない．

一方，FEP\cite{friston2010}やMDL\cite{grunwald2007}は，それぞれ「予測誤差の最小化」，
「記述長の最小化」という強い理論的枠組みを提供する．
geDIGはこれらを操作的に橋渡しし，構造編集の価値を単一スカラーで評価する点に特徴がある．
本稿の位置づけは，Attention解析を\textbf{構造評価のゲージ}として再定義する試みである．

%% ===========================================
\section{Attentionへの適用}
%% ===========================================

\subsection{AttentionからグラフへのMapping}

1つのAttentionヘッドの重み行列 $\mathbf{A} \in \mathbb{R}^{L \times L}$（$L$はシーケンス長）から有向グラフ $G = (V, E)$ を構築する．

\begin{itemize}
\item \textbf{頂点}：$V = \{1, \ldots, L\}$（トークン位置）
\item \textbf{辺}：$E = \{(i,j) \mid A_{ij} > \tau\}$（閾値 $\tau$ を超える接続）
\end{itemize}

閾値 $\tau$ は上位10パーセンタイルとし，Padトークンおよびcausal mask（GPT系）を適用後に評価する．

\subsection{各項の計算方法}

Attention適用での各項の計算を以下のように定める：

\begin{itemize}
\item $\gednorm$：エッジ密度 $= |E| / L^2$
\item $\Delta H_{\mathrm{norm}}$：Attention分布のシャノンエントロピーを $\log L^2$ で正規化
\item $\Delta \mathrm{SP}_{\mathrm{rel}}$：最大弱連結成分での平均最短路長の相対ゲイン
\end{itemize}

パラメータは $\lambda = 1.0$，$\gamma = 0.5$ に固定した．
シーケンスが長い場合，最短路計算はサンプリング（200ペア）で近似する．

\subsection{閾値設定と感度}

Attentionからグラフを構成する際の閾値 $\tau$ は結果の符号や分布に影響しうる．
本研究では\textbf{パーセンタイル閾値}（上位10\%）を採用し，モデル間・層間での比較が安定することを優先した．
固定の絶対閾値では，スケール差により$\Delta F$の符号が反転することがあるため，補助的な分析に留める．

\subsection{仮説：Transformer推論は自由エネルギー最小化的挙動}

上記の枠組みに基づき，以下の仮説を立てる：

\noindent\textbf{H1}：実Attentionは，ランダム/一様ベースラインより高いF値（0に近い＝より構造化）を示す．

\noindent\textbf{H2}：深層ほどF値が上昇（0に接近）し，情報が「探索相」から「構造相」へ移行する．

\noindent\textbf{H3}：Fを損失に組み込んだ学習は，下流タスク性能に因果的な影響を与える可能性がある．

\subsection{ベースライン}

比較のため以下のベースラインを設定した：
(1) \textbf{Random}：ランダム行列，
(2) \textbf{Uniform}：一様分布，
(3) \textbf{Local}：窓幅$w=5$の局所Attention，
(4) \textbf{Diagonal}：対角成分のみ．

%% ===========================================
\section{実験1：記述的分析}
%% ===========================================

\subsection{実験設定}

モデルとしてbert-base-uncased\cite{devlin2019bert}（12層，12ヘッド）およびgpt2\cite{radford2019gpt2}（12層，12ヘッド）を使用した．
Wikitext\cite{merity2017wikitext}から抽出した200件の短文サンプル（最大長512トークン）で評価し，各層・各ヘッドのF値を算出した（計 $200 \times 12 \times 12 = 28,800$ サンプル/モデル）．
追加検証としてLlama 3.0/3.1（各32層，32ヘッド）とPhi-3-mini-4k-instructを同様に評価した\cite{meta2024llama3,meta2024llama31,phi32024}．
Wikitextの短文63件（最大120文字，最大256トークン）を用い，計 $63 \times 32 \times 32 = 64,512$ サンプル/モデルを計測した．

\subsection{結果1：Real vs Random（H1の検証）}

表\ref{tab:delta_baselines}に平均$\Delta F$（Real - Baseline）を示す．
BERT/GPT-2ではRandomのみ計測したが，両モデルで実Attentionは有意に高いF値（0に近い＝より構造化）を示す傾向があり，効果量は$d > 2.0$と大きかった（$p < 0.001$）．
ここではF値そのものを示すが，評価は$\Delta F$（Real-Random）の符号と大きさに基づいて行う．

\begin{table}[t]
\centering
\caption{平均$\Delta F$（Real - Baseline，数値は小数第3位で丸め）}
\label{tab:delta_baselines}
\scriptsize
\begin{tabular}{lcccc}
\toprule
モデル & $\Delta F_{\mathrm{rand}}$ & $\Delta F_{\mathrm{uni}}$ & $\Delta F_{\mathrm{local}}$ & $\Delta F_{\mathrm{diag}}$ \\
\midrule
BERT & +0.11 & -- & -- & -- \\
GPT-2 & +0.10 & -- & -- & -- \\
Llama 3.0 & +0.116 & +0.108 & +0.190 & +0.074 \\
Llama 3.1 & +0.116 & +0.108 & +0.190 & +0.074 \\
Phi-3 & +0.060 & +0.052 & +0.144 & +0.018 \\
\bottomrule
\end{tabular}
\end{table}

F値の差 $\Delta F \approx 0.11$ は，操作的には「仕事」に相当する量として解釈できるが，本稿では比喩的解釈に留める．
これはモデルがランダムノイズから「秩序」を獲得したことを示す一つの定量的示唆である．

追加検証では，Llama 3.0/3.1で$\Delta F_{\mathrm{random}} \approx +0.116$（正の比率$\approx 0.995$）となり，BERT/GPT-2と同傾向が再現された．
Phi-3では$\Delta F_{\mathrm{random}} \approx +0.060$（正の比率$\approx 0.73$）と効果が弱いが，Uniform/Local/Diagonalの各ベースラインでも符号は維持された．

\textbf{H1は概ね支持された}．

\subsection{結果2：層別の遷移（H2の検証）}

図\ref{fig:layer_f}にBERTの層別F値分布を示す．

\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{figs/layer_wise_f.png}
\caption{BERTの層別F値分布（箱ひげ）．赤点線はRandom baselineの平均．}
\label{fig:layer_f}
\end{figure}

\begin{itemize}
\item \textbf{Layer 0}（$F \approx -0.34$）：高エントロピー，「探索相」
\item \textbf{Layer 1--2}（$F \approx -0.24$）：急激な上昇，「遷移」
\item \textbf{Layer 3+}（$F \approx -0.25$）：プラトー，「構造相」
\end{itemize}

これは物理における相転移（気体$\to$結晶）に類似した変化とみなせるが，ここでは比喩として扱う．
浅層では多様な情報を広く収集し（探索），深層では関連情報を効率的に統合する（構造化）と解釈できる．

追加検証では，Llama 3.0/3.1は全層で$\Delta F_{\mathrm{random}} > 0$だった一方，Phi-3はLayer 0で$\Delta F_{\mathrm{random}} < 0$となり，浅層の探索性が強い可能性がある．

\textbf{H2は概ね支持された}．

\subsection{結果3：ヘッド多様性}

同一層内でもヘッド間でF値に差異が見られた．
これは各ヘッドが異なる熱力学的状態を持ち，\textbf{マルチエージェント的}に振る舞う可能性を示唆する．
geDIGはヘッドの「役割」を定量的に診断する指標となりうる．

%% ===========================================
\section{実験2：因果的示唆（F正則化）}
%% ===========================================

\subsection{動機}

実験1は相関を示すが因果は示さない．
そこで，\textbf{Fを損失項に組み込んだ学習}が性能に与える影響を検証し，geDIGの因果的示唆を検討する．

\subsection{実験設定}

DistilBERT\cite{sanh2019distilbert}をSST-2\cite{socher2013sst}（感情分析）でfine-tuningし，損失関数を以下のように設定した：
\begin{equation}
L_{\mathrm{total}} = L_{\mathrm{CE}} + \alpha \cdot F_{\mathrm{mean}}
\end{equation}

ここで $F_{\mathrm{mean}}$ は全層・全ヘッドのF値の平均である．
$\alpha \in \{0, 0.001, 0.01, 0.1, 1.0\}$ をsweepし，各設定で3シード（42, 123, 456）の平均を報告する．
学習データ2000件，評価データ500件，3エポック，バッチサイズ16で実験した．

\subsection{結果}

表\ref{tab:f_reg}にF正則化の結果を示す．

\begin{table}[t]
\centering
\caption{F正則化の結果（3シード平均）}
\label{tab:f_reg}
\small
\begin{tabular}{lcc}
\toprule
$\alpha$ & Accuracy (\%) & Final $F$ \\
\midrule
0 (baseline) & $86.00 \pm 0.53$ & $-0.448$ \\
\textbf{0.001} & $\mathbf{86.33 \pm 0.46}$ & $-0.447$ \\
0.01 & $86.27 \pm 0.58$ & $-0.452$ \\
0.1 & $85.93 \pm 1.01$ & $-0.466$ \\
1.0 & $78.93 \pm 2.00$ & $-0.515$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\columnwidth]{figs/f_reg_alpha_accuracy.png}
\hfill
\includegraphics[width=0.48\columnwidth]{figs/f_reg_alpha_f.png}
\caption{F正則化の強さ$\alpha$と指標の関係．左：精度（弱い正則化で改善）．右：最終F（強い正則化ほど低下）．}
\label{fig:f_reg_alpha}
\end{figure}

精度は逆U字カーブを描き，$\alpha = 0.001$で最高精度 \textbf{86.33\%}（+0.33\%），$\alpha = 1.0$で78.93\%（-7\%）となった．

この結果は以下を示す：
(1) \textbf{弱いF正則化は改善傾向}：損失にFを組み込むことで性能向上に寄与，
(2) \textbf{強すぎると有害}：過度の正則化はタスク固有の情報を損なう，
(3) \textbf{geDIGは訓練目的の候補}：Fの寄与量を制御することで性能に影響を与えうる．

\textbf{H3を支持する傾向が得られた}．

%% ===========================================
\section{考察}
%% ===========================================

\subsection{理論的含意}

本研究の結果は，Transformer推論を「自由エネルギー最小化的プロセス」として解釈する視点を支持する可能性を示す．

\begin{itemize}
\item \textbf{各層}：情報の遷移（探索相$\to$構造相）を実現
\item \textbf{各ヘッド}：異なる熱力学的状態を持つマルチエージェント的挙動
\item \textbf{geDIG F}：内部状態を追跡するゲージ
\end{itemize}

これはAttentionを「情報の流れ」としてだけでなく，「構造の形成プロセス」として理解する新しい視点を提供する．
Phi-3で効果が弱いのは，蒸留や学習データ特性により注意分布の初期エントロピーが低く，$\Delta F$が縮小した可能性がある．

\subsection{限界と今後の課題}

本研究には以下の限界がある：

\begin{itemize}
\item \textbf{規模}：DistilBERT + SST-2という小規模設定
\item \textbf{効果量}：+0.33\%は統計的に有意だが実用的には小さい
\item \textbf{統計}：層/ヘッド/文が入れ子構造であり，独立性仮定が厳密ではない
\item \textbf{理論}：F正則化が「なぜ」効くかの厳密な説明は未確立
\end{itemize}

今後の課題として，大規模モデル（GPT-4クラス）での検証，多様なタスクでの再現性確認，およびgeDIGと情報理論的指標（mutual informationなど）との関係解明が挙げられる．

%% ===========================================
\section{結論}
%% ===========================================

本研究では，geDIGゲージによるAttention品質の熱力学的評価手法を提案した．
FEP-MDL橋渡しという理論的背景のもと，(1) 実AttentionがRandomより構造化されていること（$d \approx 2.3$），(2) 層別の遷移的挙動，(3) F正則化による微小な性能向上（+0.33\%）を示した．
さらにLlama 3.0/3.1で同傾向が再現される一方，Phi-3では効果が弱いことを確認した．

これらの結果は，Transformerを「自由エネルギー最小化的に解釈する枠組み」を補強し，Attention機構の設計・最適化に新しい視点を与えるものである．

%% ===========================================
%% 参考文献
%% ===========================================
\begin{thebibliography}{99}

\bibitem{vaswani2017}
Vaswani, A., et al.: Attention Is All You Need, NeurIPS (2017).

\bibitem{devlin2019bert}
Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, NAACL-HLT (2019).

\bibitem{radford2019gpt2}
Radford, A., et al.: Language Models are Unsupervised Multitask Learners, OpenAI Technical Report (2019).

\bibitem{friston2010}
Friston, K.: The free-energy principle: a unified brain theory?, Nature Reviews Neuroscience, Vol.11, pp.127--138 (2010).

\bibitem{grunwald2007}
Gr\"{u}nwald, P.~D.: The Minimum Description Length Principle, MIT Press (2007).

\bibitem{merity2017wikitext}
Merity, S., Xiong, C., Bradbury, J., Socher, R.: Pointer Sentinel Mixture Models, ICLR (2017).

\bibitem{clark2019}
Clark, K., et al.: What Does BERT Look At? An Analysis of BERT's Attention, BlackboxNLP (2019).

\bibitem{voita2019}
Voita, E., et al.: Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned, ACL (2019).

\bibitem{sanh2019distilbert}
Sanh, V., Debut, L., Chaumond, J., Wolf, T.: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, arXiv:1910.01108 (2019).

\bibitem{socher2013sst}
Socher, R., et al.: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank, EMNLP (2013).

\bibitem{lewis2020rag}
Lewis, P., et al.: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, NeurIPS (2020).

\bibitem{meta2024llama3}
Meta AI: Llama 3 8B Model Card, Hugging Face (2024), \url{https://huggingface.co/meta-llama/Meta-Llama-3-8B}.

\bibitem{meta2024llama31}
Meta AI: Llama 3.1 8B Model Card, Hugging Face (2024), \url{https://huggingface.co/meta-llama/Llama-3.1-8B}.

\bibitem{phi32024}
Microsoft: Phi-3 Mini 4K Instruct Model Card, Hugging Face (2024), \url{https://huggingface.co/microsoft/Phi-3-mini-4k-instruct}.

\end{thebibliography}

\end{document}
