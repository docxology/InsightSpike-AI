{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F-Regularization Large-Scale Experiment\n",
    "\n",
    "## Goal\n",
    "Validate the causal hypothesis at scale: **Does minimizing geDIG F during training improve performance across multiple models and tasks?**\n",
    "\n",
    "## Experiment Matrix\n",
    "- **Models**: DistilBERT, BERT-base, RoBERTa-base\n",
    "- **Tasks**: SST-2, MRPC, CoLA, QNLI (GLUE subset)\n",
    "- **α sweep**: [0, 0.001, 0.01, 0.1]\n",
    "- **Seeds**: [42, 123, 456, 789, 1024]\n",
    "\n",
    "## Expected Runtime\n",
    "- Full sweep: ~8-12 hours on T4/V100\n",
    "- Single task/model: ~30-60 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers datasets accelerate scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy import stats\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Differentiable geDIG Calculator\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class DifferentiableGeDIG:\n",
    "    \"\"\"Computes geDIG F in a differentiable manner for backpropagation.\"\"\"\n",
    "    lambda_param: float = 1.0\n",
    "    gamma: float = 0.5\n",
    "    temperature: float = 0.1\n",
    "    percentile: float = 0.9\n",
    "    max_path_length: int = 4\n",
    "\n",
    "    def compute_F(self, attention: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
    "        batch_size, num_heads, seq_len, _ = attention.shape\n",
    "        if attention_mask is not None:\n",
    "            mask_2d = attention_mask.unsqueeze(1).unsqueeze(2) * attention_mask.unsqueeze(1).unsqueeze(3)\n",
    "            attention = attention * mask_2d.float()\n",
    "        delta_epc = self._compute_soft_density(attention)\n",
    "        delta_h = self._compute_entropy(attention, attention_mask)\n",
    "        delta_sp = self._compute_soft_path_efficiency(attention, attention_mask)\n",
    "        F_values = delta_epc - self.lambda_param * (delta_h + self.gamma * delta_sp)\n",
    "        return {\"F\": F_values, \"F_mean\": F_values.mean(), \"delta_epc\": delta_epc, \"delta_h\": delta_h, \"delta_sp\": delta_sp}\n",
    "\n",
    "    def _compute_soft_density(self, attention: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, num_heads, seq_len, _ = attention.shape\n",
    "        attn_flat = attention.view(batch_size, num_heads, -1)\n",
    "        k = int(self.percentile * seq_len * seq_len)\n",
    "        threshold = torch.kthvalue(attn_flat, k, dim=-1).values.unsqueeze(-1).unsqueeze(-1)\n",
    "        edge_probs = torch.sigmoid((attention - threshold) / self.temperature)\n",
    "        return edge_probs.sum(dim=(-2, -1)) / (seq_len * seq_len)\n",
    "\n",
    "    def _compute_entropy(self, attention: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        batch_size, num_heads, seq_len, _ = attention.shape\n",
    "        attn_flat = attention.view(batch_size, num_heads, -1)\n",
    "        attn_norm = attn_flat / (attn_flat.sum(dim=-1, keepdim=True) + 1e-10)\n",
    "        entropy = -(attn_norm * torch.log(attn_norm + 1e-10)).sum(dim=-1)\n",
    "        if attention_mask is not None:\n",
    "            valid_count = attention_mask.sum(dim=-1).float()\n",
    "            max_entropy = torch.log(valid_count * valid_count + 1e-10).unsqueeze(1)\n",
    "        else:\n",
    "            max_entropy = math.log(seq_len * seq_len)\n",
    "        return entropy / (max_entropy + 1e-10)\n",
    "\n",
    "    def _compute_soft_path_efficiency(self, attention: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        batch_size, num_heads, seq_len, _ = attention.shape\n",
    "        attn_flat = attention.view(batch_size, num_heads, -1)\n",
    "        k = int(self.percentile * seq_len * seq_len)\n",
    "        threshold = torch.kthvalue(attn_flat, k, dim=-1).values.unsqueeze(-1).unsqueeze(-1)\n",
    "        adj = torch.sigmoid((attention - threshold) / self.temperature)\n",
    "        eye = torch.eye(seq_len, device=attention.device).unsqueeze(0).unsqueeze(0)\n",
    "        adj = adj + eye\n",
    "        path_efficiency = torch.zeros(batch_size, num_heads, device=attention.device)\n",
    "        adj_power = adj.clone()\n",
    "        for path_len in range(1, self.max_path_length + 1):\n",
    "            if path_len > 1:\n",
    "                adj_power = torch.clamp(torch.matmul(adj_power, adj), 0, 1)\n",
    "            path_efficiency = path_efficiency + (1.0 / path_len) * (adj_power > 0.5).float().mean(dim=(-2, -1))\n",
    "        return path_efficiency / self.max_path_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# F-Regularized Model and Trainer\n",
    "# ============================================================================\n",
    "\n",
    "class FRegularizedModel(nn.Module):\n",
    "    \"\"\"Wrapper that adds geDIG F regularization to the loss.\"\"\"\n",
    "    def __init__(self, base_model: nn.Module, alpha: float = 0.1, gedig_config: Optional[Dict[str, Any]] = None):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.alpha = alpha\n",
    "        self.gedig = DifferentiableGeDIG(**(gedig_config or {}))\n",
    "        self._last_gedig_metrics: Optional[Dict[str, float]] = None\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None, \n",
    "                labels: Optional[torch.Tensor] = None, **kwargs) -> SequenceClassifierOutput:\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask, \n",
    "                                   labels=labels, output_attentions=True, **kwargs)\n",
    "        if labels is not None and self.alpha > 0:\n",
    "            f_values = [self.gedig.compute_F(layer_attn, attention_mask)[\"F_mean\"] \n",
    "                       for layer_attn in outputs.attentions]\n",
    "            f_mean = torch.stack(f_values).mean()\n",
    "            total_loss = outputs.loss + self.alpha * f_mean\n",
    "            self._last_gedig_metrics = {\n",
    "                \"f_mean\": f_mean.item(), \n",
    "                \"ce_loss\": outputs.loss.item(), \n",
    "                \"total_loss\": total_loss.item()\n",
    "            }\n",
    "            return SequenceClassifierOutput(loss=total_loss, logits=outputs.logits, \n",
    "                                            hidden_states=None, attentions=None)\n",
    "        return SequenceClassifierOutput(loss=outputs.loss, logits=outputs.logits, \n",
    "                                        hidden_states=None, attentions=None)\n",
    "\n",
    "\n",
    "class FRegularizedTrainer(Trainer):\n",
    "    \"\"\"Trainer with geDIG metric logging.\"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        if hasattr(model, \"_last_gedig_metrics\") and model._last_gedig_metrics:\n",
    "            self.log(model._last_gedig_metrics)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Task Configurations\n",
    "# ============================================================================\n",
    "\n",
    "TASK_CONFIGS = {\n",
    "    \"sst2\": {\n",
    "        \"dataset\": (\"glue\", \"sst2\"),\n",
    "        \"text_field\": \"sentence\",\n",
    "        \"num_labels\": 2,\n",
    "        \"metric\": \"accuracy\",\n",
    "    },\n",
    "    \"mrpc\": {\n",
    "        \"dataset\": (\"glue\", \"mrpc\"),\n",
    "        \"text_field\": [\"sentence1\", \"sentence2\"],\n",
    "        \"num_labels\": 2,\n",
    "        \"metric\": \"f1\",\n",
    "    },\n",
    "    \"cola\": {\n",
    "        \"dataset\": (\"glue\", \"cola\"),\n",
    "        \"text_field\": \"sentence\",\n",
    "        \"num_labels\": 2,\n",
    "        \"metric\": \"matthews_correlation\",\n",
    "    },\n",
    "    \"qnli\": {\n",
    "        \"dataset\": (\"glue\", \"qnli\"),\n",
    "        \"text_field\": [\"question\", \"sentence\"],\n",
    "        \"num_labels\": 2,\n",
    "        \"metric\": \"accuracy\",\n",
    "    },\n",
    "}\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    \"distilbert\": \"distilbert-base-uncased\",\n",
    "    \"bert\": \"bert-base-uncased\",\n",
    "    \"roberta\": \"roberta-base\",\n",
    "}\n",
    "\n",
    "# Experiment settings\n",
    "ALPHAS = [0.0, 0.001, 0.01, 0.1]\n",
    "SEEDS = [42, 123, 456, 789, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Metrics Computation\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef\n",
    "\n",
    "def compute_metrics(pred, metric_name=\"accuracy\"):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    \n",
    "    if metric_name == \"accuracy\":\n",
    "        return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "    elif metric_name == \"f1\":\n",
    "        return {\n",
    "            \"f1\": f1_score(labels, preds),\n",
    "            \"accuracy\": accuracy_score(labels, preds),\n",
    "        }\n",
    "    elif metric_name == \"matthews_correlation\":\n",
    "        return {\n",
    "            \"matthews_correlation\": matthews_corrcoef(labels, preds),\n",
    "            \"accuracy\": accuracy_score(labels, preds),\n",
    "        }\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "\n",
    "\n",
    "def compute_final_gedig_metrics(model, eval_dataset, tokenizer, data_collator):\n",
    "    \"\"\"Compute geDIG metrics on eval set.\"\"\"\n",
    "    from torch.utils.data import DataLoader\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(eval_dataset, batch_size=32, collate_fn=data_collator)\n",
    "    gedig = DifferentiableGeDIG()\n",
    "    all_f = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "            base = model.base_model if hasattr(model, \"base_model\") else model\n",
    "            outputs = base(input_ids=batch[\"input_ids\"], attention_mask=batch.get(\"attention_mask\"), \n",
    "                          output_attentions=True)\n",
    "            for layer_attn in outputs.attentions:\n",
    "                metrics = gedig.compute_F(layer_attn, batch.get(\"attention_mask\"))\n",
    "                all_f.append(metrics[\"F\"].mean().item())\n",
    "    \n",
    "    return {\"f_mean\": np.mean(all_f), \"f_std\": np.std(all_f)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Single Experiment Runner\n",
    "# ============================================================================\n",
    "\n",
    "def run_single_experiment(\n",
    "    model_name: str,\n",
    "    task_name: str,\n",
    "    alpha: float,\n",
    "    seed: int,\n",
    "    max_train_samples: Optional[int] = None,\n",
    "    max_eval_samples: Optional[int] = None,\n",
    "    epochs: int = 3,\n",
    "    batch_size: int = 16,\n",
    "    learning_rate: float = 2e-5,\n",
    "    output_dir: Optional[Path] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Run a single F-regularization experiment.\"\"\"\n",
    "    set_seed(seed)\n",
    "    \n",
    "    task_config = TASK_CONFIGS[task_name]\n",
    "    model_path = MODEL_CONFIGS[model_name]\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = Path(f\"results/{model_name}/{task_name}/alpha_{alpha}_seed_{seed}\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Model: {model_name} | Task: {task_name} | α: {alpha} | Seed: {seed}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load dataset\n",
    "    ds_name, ds_config = task_config[\"dataset\"]\n",
    "    train_split = \"train\" if max_train_samples is None else f\"train[:{max_train_samples}]\"\n",
    "    eval_split = \"validation\" if max_eval_samples is None else f\"validation[:{max_eval_samples}]\"\n",
    "    \n",
    "    ds_train = load_dataset(ds_name, ds_config, split=train_split)\n",
    "    ds_eval = load_dataset(ds_name, ds_config, split=eval_split)\n",
    "    \n",
    "    print(f\"Train: {len(ds_train)} samples | Eval: {len(ds_eval)} samples\")\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    text_field = task_config[\"text_field\"]\n",
    "    \n",
    "    if isinstance(text_field, list):\n",
    "        tokenize_fn = lambda ex: tokenizer(ex[text_field[0]], ex[text_field[1]], \n",
    "                                           truncation=True, max_length=128)\n",
    "    else:\n",
    "        tokenize_fn = lambda ex: tokenizer(ex[text_field], truncation=True, max_length=128)\n",
    "    \n",
    "    train_ds = ds_train.map(tokenize_fn, batched=True)\n",
    "    eval_ds = ds_eval.map(tokenize_fn, batched=True)\n",
    "    \n",
    "    # Remove unused columns\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\", \"label\"}\n",
    "    train_ds = train_ds.remove_columns([c for c in train_ds.column_names if c not in keep_cols])\n",
    "    eval_ds = eval_ds.remove_columns([c for c in eval_ds.column_names if c not in keep_cols])\n",
    "    train_ds = train_ds.with_format(\"torch\")\n",
    "    eval_ds = eval_ds.with_format(\"torch\")\n",
    "    \n",
    "    # Load model\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path, num_labels=task_config[\"num_labels\"]\n",
    "    )\n",
    "    model = FRegularizedModel(base_model, alpha=alpha) if alpha > 0 else base_model\n",
    "    \n",
    "    # Training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "        eval_strategy=\"epoch\",\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"no\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=0.01,\n",
    "        report_to=[],\n",
    "        seed=seed,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    metric_name = task_config[\"metric\"]\n",
    "    \n",
    "    trainer = FRegularizedTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=lambda p: compute_metrics(p, metric_name),\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    train_result = trainer.train()\n",
    "    eval_result = trainer.evaluate()\n",
    "    \n",
    "    # Final geDIG metrics\n",
    "    final_f = compute_final_gedig_metrics(model, eval_ds, tokenizer, data_collator)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Compile result\n",
    "    result = {\n",
    "        \"model\": model_name,\n",
    "        \"task\": task_name,\n",
    "        \"alpha\": alpha,\n",
    "        \"seed\": seed,\n",
    "        \"train_samples\": len(ds_train),\n",
    "        \"eval_samples\": len(ds_eval),\n",
    "        \"epochs\": epochs,\n",
    "        \"metric_name\": metric_name,\n",
    "        \"eval_metric\": eval_result.get(f\"eval_{metric_name}\"),\n",
    "        \"eval_accuracy\": eval_result.get(\"eval_accuracy\"),\n",
    "        \"eval_loss\": eval_result.get(\"eval_loss\"),\n",
    "        \"final_f_mean\": final_f[\"f_mean\"],\n",
    "        \"final_f_std\": final_f[\"f_std\"],\n",
    "        \"runtime_seconds\": elapsed,\n",
    "    }\n",
    "    \n",
    "    # Save\n",
    "    (output_dir / \"result.json\").write_text(json.dumps(result, indent=2))\n",
    "    \n",
    "    print(f\"Result: {metric_name}={result['eval_metric']:.4f}, F={final_f['f_mean']:.4f}, time={elapsed:.1f}s\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Large-Scale Experiment Runner\n",
    "# ============================================================================\n",
    "\n",
    "def run_large_scale_experiment(\n",
    "    models: List[str] = [\"distilbert\", \"bert\", \"roberta\"],\n",
    "    tasks: List[str] = [\"sst2\", \"mrpc\", \"cola\", \"qnli\"],\n",
    "    alphas: List[float] = ALPHAS,\n",
    "    seeds: List[int] = SEEDS,\n",
    "    max_train_samples: Optional[int] = None,  # None = full dataset\n",
    "    max_eval_samples: Optional[int] = None,\n",
    "    epochs: int = 3,\n",
    "    output_dir: Path = Path(\"results\"),\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Run large-scale F-regularization experiment.\"\"\"\n",
    "    \n",
    "    total_experiments = len(models) * len(tasks) * len(alphas) * len(seeds)\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"# LARGE-SCALE F-REGULARIZATION EXPERIMENT\")\n",
    "    print(f\"# Models: {models}\")\n",
    "    print(f\"# Tasks: {tasks}\")\n",
    "    print(f\"# Alphas: {alphas}\")\n",
    "    print(f\"# Seeds: {seeds}\")\n",
    "    print(f\"# Total experiments: {total_experiments}\")\n",
    "    print(f\"{'#'*70}\\n\")\n",
    "    \n",
    "    all_results = []\n",
    "    experiment_idx = 0\n",
    "    \n",
    "    for model_name in models:\n",
    "        for task_name in tasks:\n",
    "            for alpha in alphas:\n",
    "                for seed in seeds:\n",
    "                    experiment_idx += 1\n",
    "                    print(f\"\\n[{experiment_idx}/{total_experiments}]\")\n",
    "                    \n",
    "                    try:\n",
    "                        result = run_single_experiment(\n",
    "                            model_name=model_name,\n",
    "                            task_name=task_name,\n",
    "                            alpha=alpha,\n",
    "                            seed=seed,\n",
    "                            max_train_samples=max_train_samples,\n",
    "                            max_eval_samples=max_eval_samples,\n",
    "                            epochs=epochs,\n",
    "                            output_dir=output_dir / model_name / task_name / f\"alpha_{alpha}_seed_{seed}\",\n",
    "                        )\n",
    "                        all_results.append(result)\n",
    "                        \n",
    "                        # Save intermediate results\n",
    "                        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "                        (output_dir / \"all_results_partial.json\").write_text(\n",
    "                            json.dumps(all_results, indent=2)\n",
    "                        )\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"ERROR: {e}\")\n",
    "                        all_results.append({\n",
    "                            \"model\": model_name, \"task\": task_name, \n",
    "                            \"alpha\": alpha, \"seed\": seed, \"error\": str(e)\n",
    "                        })\n",
    "    \n",
    "    # Save final results\n",
    "    (output_dir / \"all_results.json\").write_text(json.dumps(all_results, indent=2))\n",
    "    print(f\"\\nSaved {len(all_results)} results to {output_dir / 'all_results.json'}\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Statistical Analysis\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_results(results: List[Dict], output_dir: Path = Path(\"results\")):\n",
    "    \"\"\"Comprehensive statistical analysis of experiment results.\"\"\"\n",
    "    \n",
    "    df = pd.DataFrame([r for r in results if \"error\" not in r])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STATISTICAL ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. Overall summary by alpha\n",
    "    print(\"\\n### Overall Summary by Alpha ###\")\n",
    "    overall = df.groupby(\"alpha\").agg({\n",
    "        \"eval_accuracy\": [\"mean\", \"std\", \"count\"],\n",
    "        \"final_f_mean\": [\"mean\", \"std\"],\n",
    "    }).round(4)\n",
    "    print(overall)\n",
    "    \n",
    "    # 2. Per-task analysis\n",
    "    print(\"\\n### Per-Task Summary ###\")\n",
    "    for task in df[\"task\"].unique():\n",
    "        print(f\"\\n--- {task.upper()} ---\")\n",
    "        task_df = df[df[\"task\"] == task]\n",
    "        task_summary = task_df.groupby(\"alpha\").agg({\n",
    "            \"eval_metric\": [\"mean\", \"std\"],\n",
    "        }).round(4)\n",
    "        print(task_summary)\n",
    "    \n",
    "    # 3. Per-model analysis\n",
    "    print(\"\\n### Per-Model Summary ###\")\n",
    "    for model in df[\"model\"].unique():\n",
    "        print(f\"\\n--- {model.upper()} ---\")\n",
    "        model_df = df[df[\"model\"] == model]\n",
    "        model_summary = model_df.groupby(\"alpha\").agg({\n",
    "            \"eval_accuracy\": [\"mean\", \"std\"],\n",
    "        }).round(4)\n",
    "        print(model_summary)\n",
    "    \n",
    "    # 4. Statistical tests (t-test: best alpha vs baseline)\n",
    "    print(\"\\n### Statistical Significance Tests ###\")\n",
    "    baseline_df = df[df[\"alpha\"] == 0.0]\n",
    "    \n",
    "    for alpha in [a for a in df[\"alpha\"].unique() if a > 0]:\n",
    "        treatment_df = df[df[\"alpha\"] == alpha]\n",
    "        \n",
    "        baseline_acc = baseline_df[\"eval_accuracy\"].values\n",
    "        treatment_acc = treatment_df[\"eval_accuracy\"].values\n",
    "        \n",
    "        if len(baseline_acc) > 1 and len(treatment_acc) > 1:\n",
    "            t_stat, p_value = stats.ttest_ind(treatment_acc, baseline_acc)\n",
    "            effect_size = (treatment_acc.mean() - baseline_acc.mean()) / np.sqrt(\n",
    "                (baseline_acc.std()**2 + treatment_acc.std()**2) / 2\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nα={alpha} vs α=0 (baseline):\")\n",
    "            print(f\"  Baseline: {baseline_acc.mean():.4f} ± {baseline_acc.std():.4f}\")\n",
    "            print(f\"  Treatment: {treatment_acc.mean():.4f} ± {treatment_acc.std():.4f}\")\n",
    "            print(f\"  Improvement: {(treatment_acc.mean() - baseline_acc.mean())*100:+.2f}%\")\n",
    "            print(f\"  t-statistic: {t_stat:.3f}\")\n",
    "            print(f\"  p-value: {p_value:.4f} {'***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else ''}\")\n",
    "            print(f\"  Cohen's d: {effect_size:.3f}\")\n",
    "    \n",
    "    # 5. Find best configuration\n",
    "    print(\"\\n### Best Configurations ###\")\n",
    "    best_overall = df.groupby([\"model\", \"task\", \"alpha\"])[\"eval_metric\"].mean().reset_index()\n",
    "    for task in df[\"task\"].unique():\n",
    "        task_best = best_overall[best_overall[\"task\"] == task]\n",
    "        best_row = task_best.loc[task_best[\"eval_metric\"].idxmax()]\n",
    "        baseline_row = task_best[(task_best[\"alpha\"] == 0.0)]\n",
    "        if not baseline_row.empty:\n",
    "            baseline_val = baseline_row[\"eval_metric\"].mean()\n",
    "            improvement = (best_row[\"eval_metric\"] - baseline_val) * 100\n",
    "            print(f\"{task}: Best α={best_row['alpha']} ({best_row['model']}), \"\n",
    "                  f\"metric={best_row['eval_metric']:.4f}, improvement={improvement:+.2f}%\")\n",
    "    \n",
    "    # Save analysis\n",
    "    analysis = {\n",
    "        \"overall_summary\": overall.to_dict(),\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"total_experiments\": len(df),\n",
    "    }\n",
    "    (output_dir / \"analysis.json\").write_text(json.dumps(analysis, indent=2, default=str))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualization\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_results(df: pd.DataFrame, output_dir: Path = Path(\"results\")):\n",
    "    \"\"\"Generate comprehensive visualization of results.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Overall Alpha vs Accuracy\n",
    "    ax = axes[0, 0]\n",
    "    overall = df.groupby(\"alpha\")[\"eval_accuracy\"].agg([\"mean\", \"std\"]).reset_index()\n",
    "    ax.errorbar(range(len(overall)), overall[\"mean\"], yerr=overall[\"std\"],\n",
    "                marker=\"o\", markersize=10, linewidth=2, capsize=5)\n",
    "    ax.set_xticks(range(len(overall)))\n",
    "    ax.set_xticklabels([f\"{a}\" for a in overall[\"alpha\"]])\n",
    "    ax.set_xlabel(\"Alpha\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title(\"Overall: Alpha vs Accuracy\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    baseline = overall[overall[\"alpha\"] == 0][\"mean\"].values[0]\n",
    "    ax.axhline(y=baseline, color=\"gray\", linestyle=\"--\", alpha=0.7, label=\"Baseline\")\n",
    "    ax.legend()\n",
    "    \n",
    "    # 2. Per-Task Alpha vs Metric\n",
    "    ax = axes[0, 1]\n",
    "    for task in df[\"task\"].unique():\n",
    "        task_df = df[df[\"task\"] == task]\n",
    "        task_summary = task_df.groupby(\"alpha\")[\"eval_metric\"].mean().reset_index()\n",
    "        ax.plot(range(len(task_summary)), task_summary[\"eval_metric\"], \n",
    "                marker=\"o\", label=task, linewidth=2)\n",
    "    ax.set_xticks(range(len(ALPHAS)))\n",
    "    ax.set_xticklabels([f\"{a}\" for a in ALPHAS])\n",
    "    ax.set_xlabel(\"Alpha\")\n",
    "    ax.set_ylabel(\"Task Metric\")\n",
    "    ax.set_title(\"Per-Task: Alpha vs Metric\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Per-Model Alpha vs Accuracy\n",
    "    ax = axes[0, 2]\n",
    "    for model in df[\"model\"].unique():\n",
    "        model_df = df[df[\"model\"] == model]\n",
    "        model_summary = model_df.groupby(\"alpha\")[\"eval_accuracy\"].mean().reset_index()\n",
    "        ax.plot(range(len(model_summary)), model_summary[\"eval_accuracy\"],\n",
    "                marker=\"s\", label=model, linewidth=2)\n",
    "    ax.set_xticks(range(len(ALPHAS)))\n",
    "    ax.set_xticklabels([f\"{a}\" for a in ALPHAS])\n",
    "    ax.set_xlabel(\"Alpha\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title(\"Per-Model: Alpha vs Accuracy\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Alpha vs Final F\n",
    "    ax = axes[1, 0]\n",
    "    f_summary = df.groupby(\"alpha\")[\"final_f_mean\"].agg([\"mean\", \"std\"]).reset_index()\n",
    "    ax.errorbar(range(len(f_summary)), f_summary[\"mean\"], yerr=f_summary[\"std\"],\n",
    "                marker=\"s\", markersize=10, linewidth=2, capsize=5, color=\"orange\")\n",
    "    ax.set_xticks(range(len(f_summary)))\n",
    "    ax.set_xticklabels([f\"{a}\" for a in f_summary[\"alpha\"]])\n",
    "    ax.set_xlabel(\"Alpha\")\n",
    "    ax.set_ylabel(\"Final F (geDIG)\")\n",
    "    ax.set_title(\"Alpha vs Final F\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Accuracy vs F scatter (correlation)\n",
    "    ax = axes[1, 1]\n",
    "    scatter = ax.scatter(df[\"final_f_mean\"], df[\"eval_accuracy\"], \n",
    "                         c=[ALPHAS.index(a) for a in df[\"alpha\"]], \n",
    "                         cmap=\"viridis\", alpha=0.6, s=50)\n",
    "    # Trend line\n",
    "    z = np.polyfit(df[\"final_f_mean\"], df[\"eval_accuracy\"], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_range = np.linspace(df[\"final_f_mean\"].min(), df[\"final_f_mean\"].max(), 100)\n",
    "    corr = np.corrcoef(df[\"final_f_mean\"], df[\"eval_accuracy\"])[0, 1]\n",
    "    ax.plot(x_range, p(x_range), \"r--\", alpha=0.5, label=f\"r={corr:.3f}\")\n",
    "    ax.set_xlabel(\"Final F (geDIG)\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title(f\"Accuracy vs F Correlation (r={corr:.3f})\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax, label=\"Alpha index\")\n",
    "    \n",
    "    # 6. Improvement heatmap (model x task)\n",
    "    ax = axes[1, 2]\n",
    "    # Calculate improvement for best alpha vs baseline\n",
    "    improvements = []\n",
    "    for model in df[\"model\"].unique():\n",
    "        row = []\n",
    "        for task in df[\"task\"].unique():\n",
    "            subset = df[(df[\"model\"] == model) & (df[\"task\"] == task)]\n",
    "            baseline = subset[subset[\"alpha\"] == 0][\"eval_metric\"].mean()\n",
    "            best = subset.groupby(\"alpha\")[\"eval_metric\"].mean().max()\n",
    "            improvement = (best - baseline) * 100\n",
    "            row.append(improvement)\n",
    "        improvements.append(row)\n",
    "    \n",
    "    im = ax.imshow(improvements, cmap=\"RdYlGn\", aspect=\"auto\", vmin=-2, vmax=2)\n",
    "    ax.set_xticks(range(len(df[\"task\"].unique())))\n",
    "    ax.set_xticklabels(df[\"task\"].unique())\n",
    "    ax.set_yticks(range(len(df[\"model\"].unique())))\n",
    "    ax.set_yticklabels(df[\"model\"].unique())\n",
    "    ax.set_title(\"Improvement (%) vs Baseline\")\n",
    "    plt.colorbar(im, ax=ax, label=\"Improvement %\")\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(df[\"model\"].unique())):\n",
    "        for j in range(len(df[\"task\"].unique())):\n",
    "            ax.text(j, i, f\"{improvements[i][j]:.2f}\", ha=\"center\", va=\"center\", fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"fig_large_scale_results.png\", dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"Saved: {output_dir / 'fig_large_scale_results.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# EXPERIMENT EXECUTION\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTION 1: Quick Test (single model, single task)\n",
    "# Runtime: ~10-15 min\n",
    "# ============================================================================\n",
    "\n",
    "QUICK_TEST = True  # Set to False for full experiment\n",
    "\n",
    "if QUICK_TEST:\n",
    "    results = run_large_scale_experiment(\n",
    "        models=[\"distilbert\"],\n",
    "        tasks=[\"sst2\"],\n",
    "        alphas=[0.0, 0.001, 0.01],\n",
    "        seeds=[42, 123],\n",
    "        max_train_samples=2000,\n",
    "        max_eval_samples=500,\n",
    "        epochs=2,\n",
    "        output_dir=Path(\"results_quick\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTION 2: Medium Scale (all tasks, one model)\n",
    "# Runtime: ~2-3 hours\n",
    "# ============================================================================\n",
    "\n",
    "MEDIUM_SCALE = False  # Set to True to run\n",
    "\n",
    "if MEDIUM_SCALE:\n",
    "    results = run_large_scale_experiment(\n",
    "        models=[\"distilbert\"],\n",
    "        tasks=[\"sst2\", \"mrpc\", \"cola\", \"qnli\"],\n",
    "        alphas=ALPHAS,\n",
    "        seeds=[42, 123, 456],\n",
    "        max_train_samples=5000,\n",
    "        epochs=3,\n",
    "        output_dir=Path(\"results_medium\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTION 3: Full Scale Experiment\n",
    "# Runtime: ~8-12 hours (recommend A100/V100)\n",
    "# ============================================================================\n",
    "\n",
    "FULL_SCALE = False  # Set to True to run\n",
    "\n",
    "if FULL_SCALE:\n",
    "    results = run_large_scale_experiment(\n",
    "        models=[\"distilbert\", \"bert\", \"roberta\"],\n",
    "        tasks=[\"sst2\", \"mrpc\", \"cola\", \"qnli\"],\n",
    "        alphas=ALPHAS,\n",
    "        seeds=SEEDS,\n",
    "        max_train_samples=None,  # Full dataset\n",
    "        max_eval_samples=None,\n",
    "        epochs=3,\n",
    "        output_dir=Path(\"results_full\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Analyze and Visualize Results\n",
    "# ============================================================================\n",
    "\n",
    "# Load results (adjust path based on which experiment you ran)\n",
    "result_dir = Path(\"results_quick\")  # or results_medium, results_full\n",
    "\n",
    "if (result_dir / \"all_results.json\").exists():\n",
    "    with open(result_dir / \"all_results.json\") as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    df = analyze_results(results, result_dir)\n",
    "    plot_results(df, result_dir)\n",
    "else:\n",
    "    print(f\"No results found in {result_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Download Results\n",
    "# ============================================================================\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "# Adjust based on which experiment you ran\n",
    "result_dir = \"results_quick\"  # or results_medium, results_full\n",
    "\n",
    "!zip -r f_reg_large_scale_results.zip {result_dir}/\n",
    "files.download('f_reg_large_scale_results.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Interpretation Guide\n",
    "\n",
    "### Success Criteria for \"やばい\" (Breakthrough) Level\n",
    "\n",
    "| Criterion | Threshold | Status |\n",
    "|-----------|-----------|--------|\n",
    "| Consistent improvement | α>0 beats baseline in >75% of settings | ? |\n",
    "| Statistical significance | p < 0.01 for best α vs baseline | ? |\n",
    "| Effect size | Cohen's d > 0.3 (medium effect) | ? |\n",
    "| Cross-model generalization | Works on BERT, RoBERTa, DistilBERT | ? |\n",
    "| Cross-task generalization | Works on SST-2, MRPC, CoLA, QNLI | ? |\n",
    "\n",
    "### If Successful\n",
    "- geDIG F is a **trainable objective** for Transformer optimization\n",
    "- Opens path to **Attention-free architectures** based on graph principles\n",
    "- Publishable at ACL/EMNLP/NeurIPS level"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
