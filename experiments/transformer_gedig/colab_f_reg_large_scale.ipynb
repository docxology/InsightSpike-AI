{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F-Regularization Large-Scale Experiment\n",
    "\n",
    "## Goal\n",
    "Validate the causal hypothesis at scale: **Does minimizing geDIG F during training improve performance across multiple models and tasks?**\n",
    "\n",
    "## Experiment Matrix\n",
    "- **Models**: DistilBERT, BERT-base, RoBERTa-base\n",
    "- **Tasks**: SST-2, MRPC, CoLA, QNLI (GLUE subset)\n",
    "- **α sweep**: [0, 0.001, 0.01, 0.1]\n",
    "- **Seeds**: [42, 123, 456, 789, 1024]\n",
    "\n",
    "## Expected Runtime\n",
    "- Full sweep: ~8-12 hours on T4/V100\n",
    "- Single task/model: ~30-60 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 17 10:58:04 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   52C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers datasets accelerate scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy import stats\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Differentiable geDIG Calculator\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class DifferentiableGeDIG:\n",
    "    \"\"\"Computes geDIG F in a differentiable manner for backpropagation.\"\"\"\n",
    "    lambda_param: float = 1.0\n",
    "    gamma: float = 0.5\n",
    "    temperature: float = 0.1\n",
    "    percentile: float = 0.9\n",
    "    max_path_length: int = 4\n",
    "\n",
    "    def compute_F(self, attention: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
    "        batch_size, num_heads, seq_len, _ = attention.shape\n",
    "        if attention_mask is not None:\n",
    "            mask_2d = attention_mask.unsqueeze(1).unsqueeze(2) * attention_mask.unsqueeze(1).unsqueeze(3)\n",
    "            attention = attention * mask_2d.float()\n",
    "        delta_epc = self._compute_soft_density(attention)\n",
    "        delta_h = self._compute_entropy(attention, attention_mask)\n",
    "        delta_sp = self._compute_soft_path_efficiency(attention, attention_mask)\n",
    "        F_values = delta_epc - self.lambda_param * (delta_h + self.gamma * delta_sp)\n",
    "        return {\"F\": F_values, \"F_mean\": F_values.mean(), \"delta_epc\": delta_epc, \"delta_h\": delta_h, \"delta_sp\": delta_sp}\n",
    "\n",
    "    def _compute_soft_density(self, attention: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, num_heads, seq_len, _ = attention.shape\n",
    "        attn_flat = attention.view(batch_size, num_heads, -1)\n",
    "        k = int(self.percentile * seq_len * seq_len)\n",
    "        threshold = torch.kthvalue(attn_flat, k, dim=-1).values.unsqueeze(-1).unsqueeze(-1)\n",
    "        edge_probs = torch.sigmoid((attention - threshold) / self.temperature)\n",
    "        return edge_probs.sum(dim=(-2, -1)) / (seq_len * seq_len)\n",
    "\n",
    "    def _compute_entropy(self, attention: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        batch_size, num_heads, seq_len, _ = attention.shape\n",
    "        attn_flat = attention.view(batch_size, num_heads, -1)\n",
    "        attn_norm = attn_flat / (attn_flat.sum(dim=-1, keepdim=True) + 1e-10)\n",
    "        entropy = -(attn_norm * torch.log(attn_norm + 1e-10)).sum(dim=-1)\n",
    "        if attention_mask is not None:\n",
    "            valid_count = attention_mask.sum(dim=-1).float()\n",
    "            max_entropy = torch.log(valid_count * valid_count + 1e-10).unsqueeze(1)\n",
    "        else:\n",
    "            max_entropy = math.log(seq_len * seq_len)\n",
    "        return entropy / (max_entropy + 1e-10)\n",
    "\n",
    "    def _compute_soft_path_efficiency(self, attention: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        batch_size, num_heads, seq_len, _ = attention.shape\n",
    "        attn_flat = attention.view(batch_size, num_heads, -1)\n",
    "        k = int(self.percentile * seq_len * seq_len)\n",
    "        threshold = torch.kthvalue(attn_flat, k, dim=-1).values.unsqueeze(-1).unsqueeze(-1)\n",
    "        adj = torch.sigmoid((attention - threshold) / self.temperature)\n",
    "        eye = torch.eye(seq_len, device=attention.device).unsqueeze(0).unsqueeze(0)\n",
    "        adj = adj + eye\n",
    "        path_efficiency = torch.zeros(batch_size, num_heads, device=attention.device)\n",
    "        adj_power = adj.clone()\n",
    "        for path_len in range(1, self.max_path_length + 1):\n",
    "            if path_len > 1:\n",
    "                adj_power = torch.clamp(torch.matmul(adj_power, adj), 0, 1)\n",
    "            path_efficiency = path_efficiency + (1.0 / path_len) * (adj_power > 0.5).float().mean(dim=(-2, -1))\n",
    "        return path_efficiency / self.max_path_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# F-Regularized Model and Trainer\n",
    "# ============================================================================\n",
    "\n",
    "class FRegularizedModel(nn.Module):\n",
    "    \"\"\"Wrapper that adds geDIG F regularization to the loss.\"\"\"\n",
    "    def __init__(self, base_model: nn.Module, alpha: float = 0.1, gedig_config: Optional[Dict[str, Any]] = None):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.alpha = alpha\n",
    "        self.gedig = DifferentiableGeDIG(**(gedig_config or {}))\n",
    "        self._last_gedig_metrics: Optional[Dict[str, float]] = None\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None, \n",
    "                labels: Optional[torch.Tensor] = None, **kwargs) -> SequenceClassifierOutput:\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask, \n",
    "                                   labels=labels, output_attentions=True, **kwargs)\n",
    "        if labels is not None and self.alpha > 0:\n",
    "            f_values = [self.gedig.compute_F(layer_attn, attention_mask)[\"F_mean\"] \n",
    "                       for layer_attn in outputs.attentions]\n",
    "            f_mean = torch.stack(f_values).mean()\n",
    "            total_loss = outputs.loss + self.alpha * f_mean\n",
    "            self._last_gedig_metrics = {\n",
    "                \"f_mean\": f_mean.item(), \n",
    "                \"ce_loss\": outputs.loss.item(), \n",
    "                \"total_loss\": total_loss.item()\n",
    "            }\n",
    "            return SequenceClassifierOutput(loss=total_loss, logits=outputs.logits, \n",
    "                                            hidden_states=None, attentions=None)\n",
    "        return SequenceClassifierOutput(loss=outputs.loss, logits=outputs.logits, \n",
    "                                        hidden_states=None, attentions=None)\n",
    "\n",
    "\n",
    "class FRegularizedTrainer(Trainer):\n",
    "    \"\"\"Trainer with geDIG metric logging.\"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        if hasattr(model, \"_last_gedig_metrics\") and model._last_gedig_metrics:\n",
    "            self.log(model._last_gedig_metrics)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Task Configurations\n",
    "# ============================================================================\n",
    "\n",
    "TASK_CONFIGS = {\n",
    "    \"sst2\": {\n",
    "        \"dataset\": (\"glue\", \"sst2\"),\n",
    "        \"text_field\": \"sentence\",\n",
    "        \"num_labels\": 2,\n",
    "        \"metric\": \"accuracy\",\n",
    "    },\n",
    "    \"mrpc\": {\n",
    "        \"dataset\": (\"glue\", \"mrpc\"),\n",
    "        \"text_field\": [\"sentence1\", \"sentence2\"],\n",
    "        \"num_labels\": 2,\n",
    "        \"metric\": \"f1\",\n",
    "    },\n",
    "    \"cola\": {\n",
    "        \"dataset\": (\"glue\", \"cola\"),\n",
    "        \"text_field\": \"sentence\",\n",
    "        \"num_labels\": 2,\n",
    "        \"metric\": \"matthews_correlation\",\n",
    "    },\n",
    "    \"qnli\": {\n",
    "        \"dataset\": (\"glue\", \"qnli\"),\n",
    "        \"text_field\": [\"question\", \"sentence\"],\n",
    "        \"num_labels\": 2,\n",
    "        \"metric\": \"accuracy\",\n",
    "    },\n",
    "}\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    \"distilbert\": \"distilbert-base-uncased\",\n",
    "    \"bert\": \"bert-base-uncased\",\n",
    "    \"roberta\": \"roberta-base\",\n",
    "}\n",
    "\n",
    "# Experiment settings\n",
    "ALPHAS = [0.0, 0.001, 0.01, 0.1]\n",
    "SEEDS = [42, 123, 456, 789, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Metrics Computation\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef\n",
    "\n",
    "def compute_metrics(pred, metric_name=\"accuracy\"):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    \n",
    "    if metric_name == \"accuracy\":\n",
    "        return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "    elif metric_name == \"f1\":\n",
    "        return {\n",
    "            \"f1\": f1_score(labels, preds),\n",
    "            \"accuracy\": accuracy_score(labels, preds),\n",
    "        }\n",
    "    elif metric_name == \"matthews_correlation\":\n",
    "        return {\n",
    "            \"matthews_correlation\": matthews_corrcoef(labels, preds),\n",
    "            \"accuracy\": accuracy_score(labels, preds),\n",
    "        }\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "\n",
    "\n",
    "def compute_final_gedig_metrics(model, eval_dataset, tokenizer, data_collator):\n",
    "    \"\"\"Compute geDIG metrics on eval set.\"\"\"\n",
    "    from torch.utils.data import DataLoader\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(eval_dataset, batch_size=32, collate_fn=data_collator)\n",
    "    gedig = DifferentiableGeDIG()\n",
    "    all_f = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "            base = model.base_model if hasattr(model, \"base_model\") else model\n",
    "            outputs = base(input_ids=batch[\"input_ids\"], attention_mask=batch.get(\"attention_mask\"), \n",
    "                          output_attentions=True)\n",
    "            for layer_attn in outputs.attentions:\n",
    "                metrics = gedig.compute_F(layer_attn, batch.get(\"attention_mask\"))\n",
    "                all_f.append(metrics[\"F\"].mean().item())\n",
    "    \n",
    "    return {\"f_mean\": np.mean(all_f), \"f_std\": np.std(all_f)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Single Experiment Runner\n",
    "# ============================================================================\n",
    "\n",
    "def run_single_experiment(\n",
    "    model_name: str,\n",
    "    task_name: str,\n",
    "    alpha: float,\n",
    "    seed: int,\n",
    "    max_train_samples: Optional[int] = None,\n",
    "    max_eval_samples: Optional[int] = None,\n",
    "    epochs: int = 3,\n",
    "    batch_size: int = 16,\n",
    "    learning_rate: float = 2e-5,\n",
    "    output_dir: Optional[Path] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Run a single F-regularization experiment.\"\"\"\n",
    "    set_seed(seed)\n",
    "    \n",
    "    task_config = TASK_CONFIGS[task_name]\n",
    "    model_path = MODEL_CONFIGS[model_name]\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = Path(f\"results/{model_name}/{task_name}/alpha_{alpha}_seed_{seed}\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Model: {model_name} | Task: {task_name} | α: {alpha} | Seed: {seed}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load dataset\n",
    "    ds_name, ds_config = task_config[\"dataset\"]\n",
    "    train_split = \"train\" if max_train_samples is None else f\"train[:{max_train_samples}]\"\n",
    "    eval_split = \"validation\" if max_eval_samples is None else f\"validation[:{max_eval_samples}]\"\n",
    "    \n",
    "    ds_train = load_dataset(ds_name, ds_config, split=train_split)\n",
    "    ds_eval = load_dataset(ds_name, ds_config, split=eval_split)\n",
    "    \n",
    "    print(f\"Train: {len(ds_train)} samples | Eval: {len(ds_eval)} samples\")\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    text_field = task_config[\"text_field\"]\n",
    "    \n",
    "    if isinstance(text_field, list):\n",
    "        tokenize_fn = lambda ex: tokenizer(ex[text_field[0]], ex[text_field[1]], \n",
    "                                           truncation=True, max_length=128)\n",
    "    else:\n",
    "        tokenize_fn = lambda ex: tokenizer(ex[text_field], truncation=True, max_length=128)\n",
    "    \n",
    "    train_ds = ds_train.map(tokenize_fn, batched=True)\n",
    "    eval_ds = ds_eval.map(tokenize_fn, batched=True)\n",
    "    \n",
    "    # Remove unused columns\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\", \"label\"}\n",
    "    train_ds = train_ds.remove_columns([c for c in train_ds.column_names if c not in keep_cols])\n",
    "    eval_ds = eval_ds.remove_columns([c for c in eval_ds.column_names if c not in keep_cols])\n",
    "    train_ds = train_ds.with_format(\"torch\")\n",
    "    eval_ds = eval_ds.with_format(\"torch\")\n",
    "    \n",
    "    # Load model\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path, num_labels=task_config[\"num_labels\"]\n",
    "    )\n",
    "    model = FRegularizedModel(base_model, alpha=alpha) if alpha > 0 else base_model\n",
    "    \n",
    "    # Training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "        eval_strategy=\"epoch\",\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"no\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=0.01,\n",
    "        report_to=[],\n",
    "        seed=seed,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    metric_name = task_config[\"metric\"]\n",
    "    \n",
    "    trainer = FRegularizedTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=lambda p: compute_metrics(p, metric_name),\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    train_result = trainer.train()\n",
    "    eval_result = trainer.evaluate()\n",
    "    \n",
    "    # Final geDIG metrics\n",
    "    final_f = compute_final_gedig_metrics(model, eval_ds, tokenizer, data_collator)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    # Compile result\n",
    "    result = {\n",
    "        \"model\": model_name,\n",
    "        \"task\": task_name,\n",
    "        \"alpha\": alpha,\n",
    "        \"seed\": seed,\n",
    "        \"train_samples\": len(ds_train),\n",
    "        \"eval_samples\": len(ds_eval),\n",
    "        \"epochs\": epochs,\n",
    "        \"metric_name\": metric_name,\n",
    "        \"eval_metric\": eval_result.get(f\"eval_{metric_name}\"),\n",
    "        \"eval_accuracy\": eval_result.get(\"eval_accuracy\"),\n",
    "        \"eval_loss\": eval_result.get(\"eval_loss\"),\n",
    "        \"final_f_mean\": final_f[\"f_mean\"],\n",
    "        \"final_f_std\": final_f[\"f_std\"],\n",
    "        \"runtime_seconds\": elapsed,\n",
    "    }\n",
    "    \n",
    "    # Save\n",
    "    (output_dir / \"result.json\").write_text(json.dumps(result, indent=2))\n",
    "    \n",
    "    print(f\"Result: {metric_name}={result['eval_metric']:.4f}, F={final_f['f_mean']:.4f}, time={elapsed:.1f}s\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Large-Scale Experiment Runner\n",
    "# ============================================================================\n",
    "\n",
    "def run_large_scale_experiment(\n",
    "    models: List[str] = [\"distilbert\", \"bert\", \"roberta\"],\n",
    "    tasks: List[str] = [\"sst2\", \"mrpc\", \"cola\", \"qnli\"],\n",
    "    alphas: List[float] = ALPHAS,\n",
    "    seeds: List[int] = SEEDS,\n",
    "    max_train_samples: Optional[int] = None,  # None = full dataset\n",
    "    max_eval_samples: Optional[int] = None,\n",
    "    epochs: int = 3,\n",
    "    output_dir: Path = Path(\"results\"),\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Run large-scale F-regularization experiment.\"\"\"\n",
    "    \n",
    "    total_experiments = len(models) * len(tasks) * len(alphas) * len(seeds)\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"# LARGE-SCALE F-REGULARIZATION EXPERIMENT\")\n",
    "    print(f\"# Models: {models}\")\n",
    "    print(f\"# Tasks: {tasks}\")\n",
    "    print(f\"# Alphas: {alphas}\")\n",
    "    print(f\"# Seeds: {seeds}\")\n",
    "    print(f\"# Total experiments: {total_experiments}\")\n",
    "    print(f\"{'#'*70}\\n\")\n",
    "    \n",
    "    all_results = []\n",
    "    experiment_idx = 0\n",
    "    \n",
    "    for model_name in models:\n",
    "        for task_name in tasks:\n",
    "            for alpha in alphas:\n",
    "                for seed in seeds:\n",
    "                    experiment_idx += 1\n",
    "                    print(f\"\\n[{experiment_idx}/{total_experiments}]\")\n",
    "                    \n",
    "                    try:\n",
    "                        result = run_single_experiment(\n",
    "                            model_name=model_name,\n",
    "                            task_name=task_name,\n",
    "                            alpha=alpha,\n",
    "                            seed=seed,\n",
    "                            max_train_samples=max_train_samples,\n",
    "                            max_eval_samples=max_eval_samples,\n",
    "                            epochs=epochs,\n",
    "                            output_dir=output_dir / model_name / task_name / f\"alpha_{alpha}_seed_{seed}\",\n",
    "                        )\n",
    "                        all_results.append(result)\n",
    "                        \n",
    "                        # Save intermediate results\n",
    "                        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "                        (output_dir / \"all_results_partial.json\").write_text(\n",
    "                            json.dumps(all_results, indent=2)\n",
    "                        )\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"ERROR: {e}\")\n",
    "                        all_results.append({\n",
    "                            \"model\": model_name, \"task\": task_name, \n",
    "                            \"alpha\": alpha, \"seed\": seed, \"error\": str(e)\n",
    "                        })\n",
    "    \n",
    "    # Save final results\n",
    "    (output_dir / \"all_results.json\").write_text(json.dumps(all_results, indent=2))\n",
    "    print(f\"\\nSaved {len(all_results)} results to {output_dir / 'all_results.json'}\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# Statistical Analysis\n# ============================================================================\n\ndef analyze_results(results: List[Dict], output_dir: Path = Path(\"results\")):\n    \"\"\"Comprehensive statistical analysis of experiment results.\"\"\"\n    \n    df = pd.DataFrame([r for r in results if \"error\" not in r])\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"STATISTICAL ANALYSIS\")\n    print(\"=\"*70)\n    \n    # 1. Overall summary by alpha\n    print(\"\\n### Overall Summary by Alpha ###\")\n    overall = df.groupby(\"alpha\").agg({\n        \"eval_accuracy\": [\"mean\", \"std\", \"count\"],\n        \"final_f_mean\": [\"mean\", \"std\"],\n    }).round(4)\n    print(overall)\n    \n    # 2. Per-task analysis\n    print(\"\\n### Per-Task Summary ###\")\n    for task in df[\"task\"].unique():\n        print(f\"\\n--- {task.upper()} ---\")\n        task_df = df[df[\"task\"] == task]\n        task_summary = task_df.groupby(\"alpha\").agg({\n            \"eval_metric\": [\"mean\", \"std\"],\n        }).round(4)\n        print(task_summary)\n    \n    # 3. Per-model analysis\n    print(\"\\n### Per-Model Summary ###\")\n    for model in df[\"model\"].unique():\n        print(f\"\\n--- {model.upper()} ---\")\n        model_df = df[df[\"model\"] == model]\n        model_summary = model_df.groupby(\"alpha\").agg({\n            \"eval_accuracy\": [\"mean\", \"std\"],\n        }).round(4)\n        print(model_summary)\n    \n    # 4. Statistical tests (t-test: best alpha vs baseline)\n    print(\"\\n### Statistical Significance Tests ###\")\n    baseline_df = df[df[\"alpha\"] == 0.0]\n    \n    stat_results = []\n    for alpha in [a for a in df[\"alpha\"].unique() if a > 0]:\n        treatment_df = df[df[\"alpha\"] == alpha]\n        \n        baseline_acc = baseline_df[\"eval_accuracy\"].values\n        treatment_acc = treatment_df[\"eval_accuracy\"].values\n        \n        if len(baseline_acc) > 1 and len(treatment_acc) > 1:\n            t_stat, p_value = stats.ttest_ind(treatment_acc, baseline_acc)\n            effect_size = (treatment_acc.mean() - baseline_acc.mean()) / np.sqrt(\n                (baseline_acc.std()**2 + treatment_acc.std()**2) / 2\n            )\n            \n            stat_results.append({\n                \"alpha\": alpha,\n                \"baseline_mean\": float(baseline_acc.mean()),\n                \"baseline_std\": float(baseline_acc.std()),\n                \"treatment_mean\": float(treatment_acc.mean()),\n                \"treatment_std\": float(treatment_acc.std()),\n                \"improvement_pct\": float((treatment_acc.mean() - baseline_acc.mean()) * 100),\n                \"t_statistic\": float(t_stat),\n                \"p_value\": float(p_value),\n                \"cohens_d\": float(effect_size),\n            })\n            \n            print(f\"\\nα={alpha} vs α=0 (baseline):\")\n            print(f\"  Baseline: {baseline_acc.mean():.4f} ± {baseline_acc.std():.4f}\")\n            print(f\"  Treatment: {treatment_acc.mean():.4f} ± {treatment_acc.std():.4f}\")\n            print(f\"  Improvement: {(treatment_acc.mean() - baseline_acc.mean())*100:+.2f}%\")\n            print(f\"  t-statistic: {t_stat:.3f}\")\n            print(f\"  p-value: {p_value:.4f} {'***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else ''}\")\n            print(f\"  Cohen's d: {effect_size:.3f}\")\n    \n    # 5. Find best configuration\n    print(\"\\n### Best Configurations ###\")\n    best_configs = []\n    best_overall = df.groupby([\"model\", \"task\", \"alpha\"])[\"eval_metric\"].mean().reset_index()\n    for task in df[\"task\"].unique():\n        task_best = best_overall[best_overall[\"task\"] == task]\n        best_row = task_best.loc[task_best[\"eval_metric\"].idxmax()]\n        baseline_row = task_best[(task_best[\"alpha\"] == 0.0)]\n        if not baseline_row.empty:\n            baseline_val = baseline_row[\"eval_metric\"].mean()\n            improvement = (best_row[\"eval_metric\"] - baseline_val) * 100\n            best_configs.append({\n                \"task\": task,\n                \"best_alpha\": float(best_row[\"alpha\"]),\n                \"best_model\": best_row[\"model\"],\n                \"best_metric\": float(best_row[\"eval_metric\"]),\n                \"improvement_pct\": float(improvement),\n            })\n            print(f\"{task}: Best α={best_row['alpha']} ({best_row['model']}), \"\n                  f\"metric={best_row['eval_metric']:.4f}, improvement={improvement:+.2f}%\")\n    \n    # Save analysis (JSON-serializable format)\n    analysis = {\n        \"statistical_tests\": stat_results,\n        \"best_configurations\": best_configs,\n        \"timestamp\": datetime.now().isoformat(),\n        \"total_experiments\": len(df),\n    }\n    (output_dir / \"analysis.json\").write_text(json.dumps(analysis, indent=2))\n    \n    return df"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Visualization\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_results(df: pd.DataFrame, output_dir: Path = Path(\"results\")):\n",
    "    \"\"\"Generate comprehensive visualization of results.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Overall Alpha vs Accuracy\n",
    "    ax = axes[0, 0]\n",
    "    overall = df.groupby(\"alpha\")[\"eval_accuracy\"].agg([\"mean\", \"std\"]).reset_index()\n",
    "    ax.errorbar(range(len(overall)), overall[\"mean\"], yerr=overall[\"std\"],\n",
    "                marker=\"o\", markersize=10, linewidth=2, capsize=5)\n",
    "    ax.set_xticks(range(len(overall)))\n",
    "    ax.set_xticklabels([f\"{a}\" for a in overall[\"alpha\"]])\n",
    "    ax.set_xlabel(\"Alpha\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title(\"Overall: Alpha vs Accuracy\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    baseline = overall[overall[\"alpha\"] == 0][\"mean\"].values[0]\n",
    "    ax.axhline(y=baseline, color=\"gray\", linestyle=\"--\", alpha=0.7, label=\"Baseline\")\n",
    "    ax.legend()\n",
    "    \n",
    "    # 2. Per-Task Alpha vs Metric\n",
    "    ax = axes[0, 1]\n",
    "    for task in df[\"task\"].unique():\n",
    "        task_df = df[df[\"task\"] == task]\n",
    "        task_summary = task_df.groupby(\"alpha\")[\"eval_metric\"].mean().reset_index()\n",
    "        ax.plot(range(len(task_summary)), task_summary[\"eval_metric\"], \n",
    "                marker=\"o\", label=task, linewidth=2)\n",
    "    ax.set_xticks(range(len(ALPHAS)))\n",
    "    ax.set_xticklabels([f\"{a}\" for a in ALPHAS])\n",
    "    ax.set_xlabel(\"Alpha\")\n",
    "    ax.set_ylabel(\"Task Metric\")\n",
    "    ax.set_title(\"Per-Task: Alpha vs Metric\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Per-Model Alpha vs Accuracy\n",
    "    ax = axes[0, 2]\n",
    "    for model in df[\"model\"].unique():\n",
    "        model_df = df[df[\"model\"] == model]\n",
    "        model_summary = model_df.groupby(\"alpha\")[\"eval_accuracy\"].mean().reset_index()\n",
    "        ax.plot(range(len(model_summary)), model_summary[\"eval_accuracy\"],\n",
    "                marker=\"s\", label=model, linewidth=2)\n",
    "    ax.set_xticks(range(len(ALPHAS)))\n",
    "    ax.set_xticklabels([f\"{a}\" for a in ALPHAS])\n",
    "    ax.set_xlabel(\"Alpha\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title(\"Per-Model: Alpha vs Accuracy\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Alpha vs Final F\n",
    "    ax = axes[1, 0]\n",
    "    f_summary = df.groupby(\"alpha\")[\"final_f_mean\"].agg([\"mean\", \"std\"]).reset_index()\n",
    "    ax.errorbar(range(len(f_summary)), f_summary[\"mean\"], yerr=f_summary[\"std\"],\n",
    "                marker=\"s\", markersize=10, linewidth=2, capsize=5, color=\"orange\")\n",
    "    ax.set_xticks(range(len(f_summary)))\n",
    "    ax.set_xticklabels([f\"{a}\" for a in f_summary[\"alpha\"]])\n",
    "    ax.set_xlabel(\"Alpha\")\n",
    "    ax.set_ylabel(\"Final F (geDIG)\")\n",
    "    ax.set_title(\"Alpha vs Final F\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Accuracy vs F scatter (correlation)\n",
    "    ax = axes[1, 1]\n",
    "    scatter = ax.scatter(df[\"final_f_mean\"], df[\"eval_accuracy\"], \n",
    "                         c=[ALPHAS.index(a) for a in df[\"alpha\"]], \n",
    "                         cmap=\"viridis\", alpha=0.6, s=50)\n",
    "    # Trend line\n",
    "    z = np.polyfit(df[\"final_f_mean\"], df[\"eval_accuracy\"], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_range = np.linspace(df[\"final_f_mean\"].min(), df[\"final_f_mean\"].max(), 100)\n",
    "    corr = np.corrcoef(df[\"final_f_mean\"], df[\"eval_accuracy\"])[0, 1]\n",
    "    ax.plot(x_range, p(x_range), \"r--\", alpha=0.5, label=f\"r={corr:.3f}\")\n",
    "    ax.set_xlabel(\"Final F (geDIG)\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title(f\"Accuracy vs F Correlation (r={corr:.3f})\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax, label=\"Alpha index\")\n",
    "    \n",
    "    # 6. Improvement heatmap (model x task)\n",
    "    ax = axes[1, 2]\n",
    "    # Calculate improvement for best alpha vs baseline\n",
    "    improvements = []\n",
    "    for model in df[\"model\"].unique():\n",
    "        row = []\n",
    "        for task in df[\"task\"].unique():\n",
    "            subset = df[(df[\"model\"] == model) & (df[\"task\"] == task)]\n",
    "            baseline = subset[subset[\"alpha\"] == 0][\"eval_metric\"].mean()\n",
    "            best = subset.groupby(\"alpha\")[\"eval_metric\"].mean().max()\n",
    "            improvement = (best - baseline) * 100\n",
    "            row.append(improvement)\n",
    "        improvements.append(row)\n",
    "    \n",
    "    im = ax.imshow(improvements, cmap=\"RdYlGn\", aspect=\"auto\", vmin=-2, vmax=2)\n",
    "    ax.set_xticks(range(len(df[\"task\"].unique())))\n",
    "    ax.set_xticklabels(df[\"task\"].unique())\n",
    "    ax.set_yticks(range(len(df[\"model\"].unique())))\n",
    "    ax.set_yticklabels(df[\"model\"].unique())\n",
    "    ax.set_title(\"Improvement (%) vs Baseline\")\n",
    "    plt.colorbar(im, ax=ax, label=\"Improvement %\")\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(df[\"model\"].unique())):\n",
    "        for j in range(len(df[\"task\"].unique())):\n",
    "            ax.text(j, i, f\"{improvements[i][j]:.2f}\", ha=\"center\", va=\"center\", fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / \"fig_large_scale_results.png\", dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"Saved: {output_dir / 'fig_large_scale_results.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# EXPERIMENT EXECUTION\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# LARGE-SCALE F-REGULARIZATION EXPERIMENT\n",
      "# Models: ['distilbert']\n",
      "# Tasks: ['sst2']\n",
      "# Alphas: [0.0, 0.001, 0.01]\n",
      "# Seeds: [42, 123]\n",
      "# Total experiments: 6\n",
      "######################################################################\n",
      "\n",
      "\n",
      "[1/6]\n",
      "\n",
      "======================================================================\n",
      "Model: distilbert | Task: sst2 | α: 0.0 | Seed: 42\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205d27998993442aa9cee3394d57cdd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fcd56abf9784f6583511eefcb8c34bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sst2/train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd797d9488b492191468c7bbfc170db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sst2/validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e827dcaa4b134ca6acd8c90a5c627cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sst2/test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289dfb2bc8f44b77a488c5062f7955df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e331d17b89f94504b401a7e0fdf767e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d22e75d79f643bd8090c1f7e2131aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2000 samples | Eval: 500 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e7b165800f48478419acb906a718df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f493661fe94dbaa24db9cd676a0027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eeb6b662ed649df9f3afd6f2571958f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435d0e6249f84940bd375e3b7e5cbe5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57b9ece399a4028bb47b88c5665c703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b6edc24c424d17be7fe052a0b6a7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ac207488a242bdb8479ac1a51f4969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:10, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.409300</td>\n",
       "      <td>0.359725</td>\n",
       "      <td>0.834000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.258500</td>\n",
       "      <td>0.348281</td>\n",
       "      <td>0.854000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DistilBertSdpaAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: accuracy=0.8540, F=-0.4404, time=36.1s\n",
      "\n",
      "[2/6]\n",
      "\n",
      "======================================================================\n",
      "Model: distilbert | Task: sst2 | α: 0.0 | Seed: 123\n",
      "======================================================================\n",
      "Train: 2000 samples | Eval: 500 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e597945cc150408ba2645a8008913342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:10, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.444000</td>\n",
       "      <td>0.363416</td>\n",
       "      <td>0.838000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.246600</td>\n",
       "      <td>0.378327</td>\n",
       "      <td>0.848000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: accuracy=0.8480, F=-0.4471, time=15.0s\n",
      "\n",
      "[3/6]\n",
      "\n",
      "======================================================================\n",
      "Model: distilbert | Task: sst2 | α: 0.001 | Seed: 42\n",
      "======================================================================\n",
      "Train: 2000 samples | Eval: 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:14, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.389800</td>\n",
       "      <td>0.373852</td>\n",
       "      <td>0.832000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.269500</td>\n",
       "      <td>0.365227</td>\n",
       "      <td>0.858000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: accuracy=0.8580, F=-0.4464, time=19.5s\n",
      "\n",
      "[4/6]\n",
      "\n",
      "======================================================================\n",
      "Model: distilbert | Task: sst2 | α: 0.001 | Seed: 123\n",
      "======================================================================\n",
      "Train: 2000 samples | Eval: 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:14, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.412000</td>\n",
       "      <td>0.374435</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.247200</td>\n",
       "      <td>0.371348</td>\n",
       "      <td>0.856000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: accuracy=0.8560, F=-0.4471, time=19.5s\n",
      "\n",
      "[5/6]\n",
      "\n",
      "======================================================================\n",
      "Model: distilbert | Task: sst2 | α: 0.01 | Seed: 42\n",
      "======================================================================\n",
      "Train: 2000 samples | Eval: 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:15, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.386400</td>\n",
       "      <td>0.370054</td>\n",
       "      <td>0.832000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.265600</td>\n",
       "      <td>0.362771</td>\n",
       "      <td>0.854000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: accuracy=0.8540, F=-0.4516, time=20.3s\n",
      "\n",
      "[6/6]\n",
      "\n",
      "======================================================================\n",
      "Model: distilbert | Task: sst2 | α: 0.01 | Seed: 123\n",
      "======================================================================\n",
      "Train: 2000 samples | Eval: 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:15, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.410200</td>\n",
       "      <td>0.359407</td>\n",
       "      <td>0.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.244800</td>\n",
       "      <td>0.368958</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: accuracy=0.8500, F=-0.4495, time=20.2s\n",
      "\n",
      "Saved 6 results to results_quick/all_results.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPTION 1: Quick Test (single model, single task)\n",
    "# Runtime: ~10-15 min\n",
    "# ============================================================================\n",
    "\n",
    "QUICK_TEST = True  # Set to False for full experiment\n",
    "\n",
    "if QUICK_TEST:\n",
    "    results = run_large_scale_experiment(\n",
    "        models=[\"distilbert\"],\n",
    "        tasks=[\"sst2\"],\n",
    "        alphas=[0.0, 0.001, 0.01],\n",
    "        seeds=[42, 123],\n",
    "        max_train_samples=2000,\n",
    "        max_eval_samples=500,\n",
    "        epochs=2,\n",
    "        output_dir=Path(\"results_quick\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTION 2: Medium Scale (all tasks, one model)\n",
    "# Runtime: ~2-3 hours\n",
    "# ============================================================================\n",
    "\n",
    "MEDIUM_SCALE = False  # Set to True to run\n",
    "\n",
    "if MEDIUM_SCALE:\n",
    "    results = run_large_scale_experiment(\n",
    "        models=[\"distilbert\"],\n",
    "        tasks=[\"sst2\", \"mrpc\", \"cola\", \"qnli\"],\n",
    "        alphas=ALPHAS,\n",
    "        seeds=[42, 123, 456],\n",
    "        max_train_samples=5000,\n",
    "        epochs=3,\n",
    "        output_dir=Path(\"results_medium\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTION 3: Full Scale Experiment\n",
    "# Runtime: ~8-12 hours (recommend A100/V100)\n",
    "# ============================================================================\n",
    "\n",
    "FULL_SCALE = False  # Set to True to run\n",
    "\n",
    "if FULL_SCALE:\n",
    "    results = run_large_scale_experiment(\n",
    "        models=[\"distilbert\", \"bert\", \"roberta\"],\n",
    "        tasks=[\"sst2\", \"mrpc\", \"cola\", \"qnli\"],\n",
    "        alphas=ALPHAS,\n",
    "        seeds=SEEDS,\n",
    "        max_train_samples=None,  # Full dataset\n",
    "        max_eval_samples=None,\n",
    "        epochs=3,\n",
    "        output_dir=Path(\"results_full\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STATISTICAL ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "### Overall Summary by Alpha ###\n",
      "      eval_accuracy               final_f_mean        \n",
      "               mean     std count         mean     std\n",
      "alpha                                                 \n",
      "0.000         0.851  0.0042     2      -0.4437  0.0048\n",
      "0.001         0.857  0.0014     2      -0.4468  0.0005\n",
      "0.010         0.852  0.0028     2      -0.4505  0.0015\n",
      "\n",
      "### Per-Task Summary ###\n",
      "\n",
      "--- SST2 ---\n",
      "      eval_metric        \n",
      "             mean     std\n",
      "alpha                    \n",
      "0.000       0.851  0.0042\n",
      "0.001       0.857  0.0014\n",
      "0.010       0.852  0.0028\n",
      "\n",
      "### Per-Model Summary ###\n",
      "\n",
      "--- DISTILBERT ---\n",
      "      eval_accuracy        \n",
      "               mean     std\n",
      "alpha                      \n",
      "0.000         0.851  0.0042\n",
      "0.001         0.857  0.0014\n",
      "0.010         0.852  0.0028\n",
      "\n",
      "### Statistical Significance Tests ###\n",
      "\n",
      "α=0.001 vs α=0 (baseline):\n",
      "  Baseline: 0.8510 ± 0.0030\n",
      "  Treatment: 0.8570 ± 0.0010\n",
      "  Improvement: +0.60%\n",
      "  t-statistic: 1.897\n",
      "  p-value: 0.1982 \n",
      "  Cohen's d: 2.683\n",
      "\n",
      "α=0.01 vs α=0 (baseline):\n",
      "  Baseline: 0.8510 ± 0.0030\n",
      "  Treatment: 0.8520 ± 0.0020\n",
      "  Improvement: +0.10%\n",
      "  t-statistic: 0.277\n",
      "  p-value: 0.8075 \n",
      "  Cohen's d: 0.392\n",
      "\n",
      "### Best Configurations ###\n",
      "sst2: Best α=0.001 (distilbert), metric=0.8570, improvement=+0.60%\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "keys must be str, int, float, bool or None, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2260006429.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mplot_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-368405805.py\u001b[0m in \u001b[0;36manalyze_results\u001b[0;34m(results, output_dir)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;34m\"total_experiments\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     }\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"analysis.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         **kw).encode(obj)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 raise TypeError(f'keys must be str, int, float, bool or None, '\n\u001b[0m\u001b[1;32m    378\u001b[0m                                 f'not {key.__class__.__name__}')\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: keys must be str, int, float, bool or None, not tuple"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Analyze and Visualize Results\n",
    "# ============================================================================\n",
    "\n",
    "# Load results (adjust path based on which experiment you ran)\n",
    "result_dir = Path(\"results_quick\")  # or results_medium, results_full\n",
    "\n",
    "if (result_dir / \"all_results.json\").exists():\n",
    "    with open(result_dir / \"all_results.json\") as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    df = analyze_results(results, result_dir)\n",
    "    plot_results(df, result_dir)\n",
    "else:\n",
    "    print(f\"No results found in {result_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Download Results\n",
    "# ============================================================================\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "# Adjust based on which experiment you ran\n",
    "result_dir = \"results_quick\"  # or results_medium, results_full\n",
    "\n",
    "!zip -r f_reg_large_scale_results.zip {result_dir}/\n",
    "files.download('f_reg_large_scale_results.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Interpretation Guide\n",
    "\n",
    "### Success Criteria for \"やばい\" (Breakthrough) Level\n",
    "\n",
    "| Criterion | Threshold | Status |\n",
    "|-----------|-----------|--------|\n",
    "| Consistent improvement | α>0 beats baseline in >75% of settings | ? |\n",
    "| Statistical significance | p < 0.01 for best α vs baseline | ? |\n",
    "| Effect size | Cohen's d > 0.3 (medium effect) | ? |\n",
    "| Cross-model generalization | Works on BERT, RoBERTa, DistilBERT | ? |\n",
    "| Cross-task generalization | Works on SST-2, MRPC, CoLA, QNLI | ? |\n",
    "\n",
    "### If Successful\n",
    "- geDIG F is a **trainable objective** for Transformer optimization\n",
    "- Opens path to **Attention-free architectures** based on graph principles\n",
    "- Publishable at ACL/EMNLP/NeurIPS level"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}