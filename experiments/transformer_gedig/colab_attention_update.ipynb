{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "288bf8af",
   "metadata": {},
   "source": [
    "# geDIG Attention Update (Modern LLMs)\n",
    "\n",
    "Goal: sample attention maps from newer LLMs (Llama 3, Phi-3) and check whether geDIG F keeps the same sign/delta vs. random.\n",
    "\n",
    "Notes:\n",
    "- Llama 3 requires an HF token and license acceptance.\n",
    "- Phi-3 is open and should run without a token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5671b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0015e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/miyauchikazuyoshi/InsightSpike-AI.git\n",
    "%cd InsightSpike-AI\n",
    "!pip -q install -U pip\n",
    "!pip -q install transformers datasets accelerate sentencepiece networkx scipy huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87c9a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# login()  # Uncomment and paste your HF token if you want to use Llama 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbea9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "LLAMA3_ID = \"meta-llama/Meta-Llama-3-8B\"\n",
    "PHI3_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "def show_status(ok: bool, title: str, detail: str = \"\"):\n",
    "    color = \"#1b8a5a\" if ok else \"#b02a37\"\n",
    "    msg = f\"<b>{title}</b>\" + (f\"<br/>{detail}\" if detail else \"\")\n",
    "    display(HTML(f\"<div style='border:1px solid {color};padding:10px;border-radius:6px;color:{color};margin:8px 0;'>{msg}</div>\"))\n",
    "\n",
    "api = HfApi()\n",
    "llama3_ok = False\n",
    "try:\n",
    "    api.model_info(LLAMA3_ID)\n",
    "    show_status(True, \"Llama 3 access OK\", f\"Model: {LLAMA3_ID}\")\n",
    "    llama3_ok = True\n",
    "except Exception as exc:\n",
    "    show_status(False, \"Llama 3 access NOT available\", f\"{type(exc).__name__}: {exc}\")\n",
    "    print(\"If you need Llama 3, accept the license on HF and run login().\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23001ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = \"bfloat16\"  # use \"float16\" for T4, \"float32\" for CPU\n",
    "ATTN_IMPL = \"eager\"  # to ensure attentions are returned\n",
    "TEXT_COUNT = 16\n",
    "LAYER_CAP = 4\n",
    "ATTN_MAX_LEN = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430b07c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "outputs = {}\n",
    "\n",
    "\n",
    "def run_model(model_id: str, tag: str):\n",
    "    out_path = f\"results/transformer_gedig/score_llm_{tag}.json\"\n",
    "    cmd = (\n",
    "        f\"python experiments/transformer_gedig/extract_and_score.py \"\n",
    "        f\"--model \"{model_id}\" \"\n",
    "        f\"--text-count {TEXT_COUNT} \"\n",
    "        f\"--layer-cap {LAYER_CAP} \"\n",
    "        f\"--attn-max-len {ATTN_MAX_LEN} \"\n",
    "        f\"--percentile 0.90 \"\n",
    "        f\"--out \"{out_path}\" \"\n",
    "        f\"--device auto --dtype {DTYPE} --device-map auto \"\n",
    "        f\"--trust-remote-code --attn-implementation {ATTN_IMPL}\"\n",
    "    )\n",
    "    print(cmd)\n",
    "    !{cmd}\n",
    "    outputs[tag] = out_path\n",
    "\n",
    "\n",
    "# Always run Phi-3\n",
    "run_model(PHI3_ID, \"phi3\")\n",
    "\n",
    "# Run Llama 3 only if access is available\n",
    "if llama3_ok:\n",
    "    run_model(LLAMA3_ID, \"llama3\")\n",
    "else:\n",
    "    print(\"Skipping Llama 3 run (no access).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4899706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(path):\n",
    "    data = json.loads(Path(path).read_text(encoding=\"utf-8\"))\n",
    "    rows = [r for r in data if not r.get(\"subgraph\")]\n",
    "    if not rows:\n",
    "        return None\n",
    "    f_real = np.array([r[\"F\"] for r in rows], dtype=float)\n",
    "    f_rand = np.array([r.get(\"baseline_F_random\") for r in rows], dtype=float)\n",
    "    delta = f_real - f_rand\n",
    "    return {\n",
    "        \"rows\": len(rows),\n",
    "        \"F_real_mean\": float(f_real.mean()),\n",
    "        \"F_rand_mean\": float(f_rand.mean()),\n",
    "        \"delta_mean\": float(delta.mean()),\n",
    "        \"delta_std\": float(delta.std()),\n",
    "        \"delta_positive_ratio\": float((delta > 0).mean()),\n",
    "    }\n",
    "\n",
    "summary = {tag: summarize(path) for tag, path in outputs.items()}\n",
    "print(json.dumps(summary, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8465f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "save_dir = Path(\"/content/drive/MyDrive/insightspike/gedig_attention\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for tag, path in outputs.items():\n",
    "    src = Path(path)\n",
    "    if src.exists():\n",
    "        shutil.copy2(src, save_dir / src.name)\n",
    "        print(\"Saved:\", save_dir / src.name)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "gedig_attention_update.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}