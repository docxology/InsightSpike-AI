{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# geDIG Attention Update (Modern LLMs)\n\nGoal: sample attention maps from newer LLMs (Llama 3.0, Llama 3.1, Phi-3) and check whether geDIG F keeps the same sign/delta vs. random.\n\nNotes:\n- Llama 3 requires an HF token and license acceptance (per model).\n- Phi-3 is open and should run without a token.\n- The notebook auto-selects dtype based on GPU (A100 -> bfloat16, T4 -> float16).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from google.colab import drive\n\ndrive.mount('/content/drive')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import os\nimport subprocess\n\nif not os.path.isdir('InsightSpike-AI'):\n    subprocess.check_call(['git', 'clone', 'https://github.com/miyauchikazuyoshi/InsightSpike-AI.git'])\n%cd InsightSpike-AI\n!pip -q install -U pip\n!pip -q install transformers datasets accelerate sentencepiece networkx scipy huggingface_hub\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from huggingface_hub import login\nfrom google.colab import userdata\n\nhf_token = None\ntry:\n    # Try to retrieve the token from Colab Secrets\n    hf_token = userdata.get('HF_TOKEN')\n    if hf_token:\n        login(token=hf_token)\n        print(\"Successfully logged in using Colab Secret 'HF_TOKEN'.\")\n    else:\n        # Fallback to interactive login\n        print(\"Secret 'HF_TOKEN' not found. Please log in interactively:\")\n        login()\nexcept Exception:\n    # If userdata is not available or fails, fallback to interactive\n    print(\"Could not access Colab Secrets. Please log in interactively:\")\n    login()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "from huggingface_hub import hf_hub_download\nfrom IPython.display import display, HTML\n\nLLAMA3_MODELS = {\n    'llama3': 'meta-llama/Meta-Llama-3-8B',\n    'llama31': 'meta-llama/Llama-3.1-8B',\n}\nPHI3_ID = 'microsoft/Phi-3-mini-4k-instruct'\n\ndef show_status(ok: bool, title: str, detail: str = ''):\n    color = '#1b8a5a' if ok else '#b02a37'\n    msg = f'<b>{title}</b>' + (f'<br/>{detail}' if detail else '')\n    display(HTML(f\"<div style='border:1px solid {color};padding:10px;border-radius:6px;color:{color};margin:8px 0;'>{msg}</div>\"))\n\nllama3_ok = {}\ntoken = hf_token if hf_token else None\nfor tag, model_id in LLAMA3_MODELS.items():\n    label = 'Llama 3.0' if tag == 'llama3' else 'Llama 3.1'\n    try:\n        hf_hub_download(repo_id=model_id, filename='config.json', token=token)\n        show_status(True, f'{label} download OK', f'Model: {model_id}')\n        llama3_ok[tag] = True\n    except Exception as exc:\n        show_status(False, f'{label} download NOT available', f'{type(exc).__name__}: {exc}')\n        llama3_ok[tag] = False\n        print(f'If you need {label}, accept the license on HF and run login().')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import subprocess\n\ndef _get_gpu_name():\n    try:\n        out = subprocess.check_output(\n            ['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'],\n            text=True,\n        ).strip()\n        return out.splitlines()[0] if out else None\n    except Exception:\n        return None\n\nGPU_NAME = _get_gpu_name()\nif GPU_NAME:\n    print('GPU:', GPU_NAME)\n    if 'A100' in GPU_NAME or 'H100' in GPU_NAME:\n        DTYPE = 'bfloat16'\n    else:\n        DTYPE = 'float16'\n    DEVICE = 'cuda'\n    DEVICE_MAP = 'auto'\nelse:\n    print('GPU: none (CPU)')\n    DTYPE = 'float32'\n    DEVICE = 'cpu'\n    DEVICE_MAP = None\n\nATTN_IMPL = 'eager'  # ensure attentions are returned\nTEXT_COUNT = 16\nLAYER_CAP = 4\nATTN_MAX_LEN = 256\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import json\nimport numpy as np\nfrom pathlib import Path\n\noutputs = {}\n\ndef run_model(model_id: str, tag: str):\n    out_path = f'results/transformer_gedig/score_llm_{tag}.json'\n    device_map_arg = f'--device-map {DEVICE_MAP}' if DEVICE_MAP else ''\n    cmd = (\n        f\"python experiments/transformer_gedig/extract_and_score.py \"\n        f\"--model '{model_id}' \"\n        f\"--text-count {TEXT_COUNT} \"\n        f\"--layer-cap {LAYER_CAP} \"\n        f\"--attn-max-len {ATTN_MAX_LEN} \"\n        f\"--percentile 0.90 \"\n        f\"--out '{out_path}' \"\n        f\"--device {DEVICE} --dtype {DTYPE} \"\n        f\"{device_map_arg} \"\n        f\"--trust-remote-code --attn-implementation {ATTN_IMPL}\"\n    )\n    print(cmd)\n    !{cmd}\n    outputs[tag] = out_path\n\n# Always run Phi-3\nrun_model(PHI3_ID, 'phi3')\n\n# Run Llama 3 models only if access is available\nfor tag, model_id in LLAMA3_MODELS.items():\n    if llama3_ok.get(tag):\n        run_model(model_id, tag)\n    else:\n        print(f'Skipping {tag} run (no access).')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def summarize(path):\n    data = json.loads(Path(path).read_text(encoding='utf-8'))\n    rows = [r for r in data if not r.get('subgraph')]\n    if not rows:\n        return None\n    f_real = np.array([r['F'] for r in rows], dtype=float)\n    f_rand = np.array([r.get('baseline_F_random') for r in rows], dtype=float)\n    delta = f_real - f_rand\n    return {\n        'rows': len(rows),\n        'F_real_mean': float(f_real.mean()),\n        'F_rand_mean': float(f_rand.mean()),\n        'delta_mean': float(delta.mean()),\n        'delta_std': float(delta.std()),\n        'delta_positive_ratio': float((delta > 0).mean()),\n    }\n\nsummary = {tag: summarize(path) for tag, path in outputs.items()}\nprint(json.dumps(summary, indent=2))\n\nsummary_path = Path('results/transformer_gedig/score_llm_update.json')\nsummary_path.parent.mkdir(parents=True, exist_ok=True)\nsummary_path.write_text(json.dumps(summary, indent=2), encoding='utf-8')\nprint('Wrote:', summary_path)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import shutil\n\nsave_dir = Path('/content/drive/MyDrive/insightspike/gedig_attention')\nsave_dir.mkdir(parents=True, exist_ok=True)\n\nfor tag, path in outputs.items():\n    src = Path(path)\n    if src.exists():\n        shutil.copy2(src, save_dir / src.name)\n        print('Saved:', save_dir / src.name)\n\nsummary_src = Path('results/transformer_gedig/score_llm_update.json')\nif summary_src.exists():\n    shutil.copy2(summary_src, save_dir / summary_src.name)\n    print('Saved:', save_dir / summary_src.name)\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "gedig_attention_update.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}