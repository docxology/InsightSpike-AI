{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "288bf8af",
      "metadata": {
        "id": "288bf8af"
      },
      "source": [
        "# geDIG Attention Update (Modern LLMs)\n",
        "\n",
        "Goal: sample attention maps from newer LLMs (Llama 3, Phi-3) and check whether geDIG F keeps the same sign/delta vs. random.\n",
        "\n",
        "Notes:\n",
        "- Llama 3 requires an HF token and license acceptance.\n",
        "- Phi-3 is open and should run without a token.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "5c5671b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c5671b5",
        "outputId": "6a6820ef-7207-4c1b-925c-4d55e227e0c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "0a0015e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a0015e5",
        "outputId": "88293604-9cb3-4e89-8d5e-f878f7d2e508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'InsightSpike-AI'...\n",
            "remote: Enumerating objects: 4103, done.\u001b[K\n",
            "remote: Counting objects: 100% (851/851), done.\u001b[K\n",
            "remote: Compressing objects: 100% (678/678), done.\u001b[K\n",
            "remote: Total 4103 (delta 242), reused 364 (delta 162), pack-reused 3252 (from 3)\u001b[K\n",
            "Receiving objects: 100% (4103/4103), 142.61 MiB | 15.93 MiB/s, done.\n",
            "Resolving deltas: 100% (1289/1289), done.\n",
            "/content/InsightSpike-AI/InsightSpike-AI/InsightSpike-AI/InsightSpike-AI\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/miyauchikazuyoshi/InsightSpike-AI.git\n",
        "%cd InsightSpike-AI\n",
        "!pip -q install -U pip\n",
        "!pip -q install transformers datasets accelerate sentencepiece networkx scipy huggingface_hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "d87c9a2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d87c9a2a",
        "outputId": "70082044-b6d5-4872-cae6-5d9156a147d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully logged in using Colab Secret 'HF_TOKEN'.\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    # Try to retrieve the token from Colab Secrets\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    if hf_token:\n",
        "        login(token=hf_token)\n",
        "        print(\"Successfully logged in using Colab Secret 'HF_TOKEN'.\")\n",
        "    else:\n",
        "        # Fallback to interactive login\n",
        "        print(\"Secret 'HF_TOKEN' not found. Please log in interactively:\")\n",
        "        login()\n",
        "except Exception:\n",
        "    # If userdata is not available or fails, fallback to interactive\n",
        "    print(\"Could not access Colab Secrets. Please log in interactively:\")\n",
        "    login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "9cbea9d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "9cbea9d6",
        "outputId": "20d4e871-5874-4e39-da4f-15dd7797d4de"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style='border:1px solid #1b8a5a;padding:10px;border-radius:6px;color:#1b8a5a;margin:8px 0;'><b>Llama 3 access OK</b><br/>Model: meta-llama/Meta-Llama-3-8B</div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import HfApi\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "LLAMA3_ID = \"meta-llama/Meta-Llama-3-8B\"\n",
        "PHI3_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "def show_status(ok: bool, title: str, detail: str = \"\"):\n",
        "    color = \"#1b8a5a\" if ok else \"#b02a37\"\n",
        "    msg = f\"<b>{title}</b>\" + (f\"<br/>{detail}\" if detail else \"\")\n",
        "    display(HTML(f\"<div style='border:1px solid {color};padding:10px;border-radius:6px;color:{color};margin:8px 0;'>{msg}</div>\"))\n",
        "\n",
        "api = HfApi()\n",
        "llama3_ok = False\n",
        "try:\n",
        "    api.model_info(LLAMA3_ID)\n",
        "    show_status(True, \"Llama 3 access OK\", f\"Model: {LLAMA3_ID}\")\n",
        "    llama3_ok = True\n",
        "except Exception as exc:\n",
        "    show_status(False, \"Llama 3 access NOT available\", f\"{type(exc).__name__}: {exc}\")\n",
        "    print(\"If you need Llama 3, accept the license on HF and run login().\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "23001ff5",
      "metadata": {
        "id": "23001ff5"
      },
      "outputs": [],
      "source": [
        "DTYPE = \"bfloat16\"  # use \"float16\" for T4, \"float32\" for CPU\n",
        "ATTN_IMPL = \"eager\"  # to ensure attentions are returned\n",
        "TEXT_COUNT = 16\n",
        "LAYER_CAP = 4\n",
        "ATTN_MAX_LEN = 256\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "430b07c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "430b07c5",
        "outputId": "f6eab014-2fe8-40d1-e306-b23772dd1e12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python experiments/transformer_gedig/extract_and_score.py --model 'microsoft/Phi-3-mini-4k-instruct' --text-count 16 --layer-cap 4 --attn-max-len 256 --percentile 0.90 --out 'results/transformer_gedig/score_llm_phi3.json' --device auto --dtype bfloat16 --device-map auto --trust-remote-code --attn-implementation eager\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "2025-12-28 11:44:14.411596: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766922254.432037    9466 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766922254.438795    9466 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766922254.454302    9466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766922254.454337    9466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766922254.454342    9466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766922254.454348    9466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-28 11:44:14.459778: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading checkpoint shards: 100% 2/2 [00:20<00:00, 10.34s/it]\n",
            "[warn] failed to load/extract for microsoft/Phi-3-mini-4k-instruct: Got unsupported ScalarType BFloat16\n",
            "[done] wrote results/transformer_gedig/score_llm_phi3.json (0 rows)\n",
            "python experiments/transformer_gedig/extract_and_score.py --model 'meta-llama/Meta-Llama-3-8B' --text-count 16 --layer-cap 4 --attn-max-len 256 --percentile 0.90 --out 'results/transformer_gedig/score_llm_llama3.json' --device auto --dtype bfloat16 --device-map auto --trust-remote-code --attn-implementation eager\n",
            "[warn] failed to load/extract for meta-llama/Meta-Llama-3-8B: You are trying to access a gated repo.\n",
            "Make sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B.\n",
            "403 Client Error. (Request ID: Root=1-6951183f-60d3aa2c22f3ad8e7428334c;9fd42bb4-7a30-48bc-8ce6-beee3b8c00f6)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\n",
            "Access to model meta-llama/Meta-Llama-3-8B is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Meta-Llama-3-8B to ask for access.\n",
            "[done] wrote results/transformer_gedig/score_llm_llama3.json (0 rows)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "outputs = {}\n",
        "\n",
        "\n",
        "def run_model(model_id: str, tag: str):\n",
        "    out_path = f\"results/transformer_gedig/score_llm_{tag}.json\"\n",
        "    cmd = (\n",
        "        f\"python experiments/transformer_gedig/extract_and_score.py \"\n",
        "        f\"--model '{model_id}' \"\n",
        "        f\"--text-count {TEXT_COUNT} \"\n",
        "        f\"--layer-cap {LAYER_CAP} \"\n",
        "        f\"--attn-max-len {ATTN_MAX_LEN} \"\n",
        "        f\"--percentile 0.90 \"\n",
        "        f\"--out '{out_path}' \"\n",
        "        f\"--device auto --dtype {DTYPE} --device-map auto \"\n",
        "        f\"--trust-remote-code --attn-implementation {ATTN_IMPL}\"\n",
        "    )\n",
        "    print(cmd)\n",
        "    !{cmd}\n",
        "    outputs[tag] = out_path\n",
        "\n",
        "\n",
        "# Always run Phi-3\n",
        "run_model(PHI3_ID, \"phi3\")\n",
        "\n",
        "# Run Llama 3 only if access is available\n",
        "if llama3_ok:\n",
        "    run_model(LLAMA3_ID, \"llama3\")\n",
        "else:\n",
        "    print(\"Skipping Llama 3 run (no access).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "4899706d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4899706d",
        "outputId": "1d12ecca-65e8-46c3-ddea-53ef72bab133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"phi3\": null,\n",
            "  \"llama3\": null\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "def summarize(path):\n",
        "    data = json.loads(Path(path).read_text(encoding=\"utf-8\"))\n",
        "    rows = [r for r in data if not r.get(\"subgraph\")]\n",
        "    if not rows:\n",
        "        return None\n",
        "    f_real = np.array([r[\"F\"] for r in rows], dtype=float)\n",
        "    f_rand = np.array([r.get(\"baseline_F_random\") for r in rows], dtype=float)\n",
        "    delta = f_real - f_rand\n",
        "    return {\n",
        "        \"rows\": len(rows),\n",
        "        \"F_real_mean\": float(f_real.mean()),\n",
        "        \"F_rand_mean\": float(f_rand.mean()),\n",
        "        \"delta_mean\": float(delta.mean()),\n",
        "        \"delta_std\": float(delta.std()),\n",
        "        \"delta_positive_ratio\": float((delta > 0).mean()),\n",
        "    }\n",
        "\n",
        "summary = {tag: summarize(path) for tag, path in outputs.items()}\n",
        "print(json.dumps(summary, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "f8465f4b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8465f4b",
        "outputId": "c529ef66-65c0-4d1e-b287-335b53cdcb76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/insightspike/gedig_attention/score_llm_phi3.json\n",
            "Saved: /content/drive/MyDrive/insightspike/gedig_attention/score_llm_llama3.json\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "save_dir = Path(\"/content/drive/MyDrive/insightspike/gedig_attention\")\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for tag, path in outputs.items():\n",
        "    src = Path(path)\n",
        "    if src.exists():\n",
        "        shutil.copy2(src, save_dir / src.name)\n",
        "        print(\"Saved:\", save_dir / src.name)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "gedig_attention_update.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}